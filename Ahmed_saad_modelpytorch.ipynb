{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch_models_scipy.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "OOU9XKLry4Ju",
        "yqVg3gtpw39p",
        "5jWDAkBdahB6",
        "v-xZPoM6rItg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1keHPjUl37f"
      },
      "source": [
        "!pip install adjustText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNGy4uggl_Uz"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib.ticker import MultipleLocator\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEZkOdBB5_iS"
      },
      "source": [
        "transfer_learning = [i for i in range(7)]\n",
        "randomly_init = [13, 15, 17, 19]\n",
        "semi_models = [7, 8, 9, 10, 11, 12, 14, 16, 18, 20]\n",
        "sequence_models = [i for i in range(21,29)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13yTuerVEv96"
      },
      "source": [
        "[models[i] for i in sequence_models]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI9iuJ_se00J"
      },
      "source": [
        "models = ['Xception-TL', 'Mobilenet-TL', 'Resnet50-TL', 'Densenet-TL', 'Nasnet-TL', 'VGG16-TL', 'VGG19-TL', 'Densenet-semi-TL', 'Mobilenet-semi-TL',\n",
        "          'Nasnet-semi-TL', 'Resnet50-semi-TL', 'VGG16-semi-TL', 'VGG19-semi-TL', 'Densenet', 'Densenet-semi', 'DPN', 'DPN-semi', 'Resnext',\n",
        "          'Resnext-semi', 'Wideresnet', 'Wideresnet-semi', 'RNN', 'RNN 2 stages silence', 'RNN 2 stages unknown', 'RNN 2 stages 3 classes',\n",
        "          'RNN-semi', 'RNN 2 stages silence-semi', 'RNN 2 stages unknown-semi', 'RNN 2 stages 3 classes-semi']\n",
        "\n",
        "parameters = [24, 4, 26, 13, 87, 15, 21, 13, 4, 87, 26, 15, 20, 0.7, 0.7, 34, 34, 34, 34, 75, 75, 4.4, 8.8, 8.8, 8.8, 4.4, 8.8, 8.8, 8.8]\n",
        "\n",
        "test_acc = [0.908, 0.954, 0.959, 0.964, 0.946, 0.963, 0.962, 0.962, 0.874, 0.945, 0.954, 0.958, 0.96, 0.958, 0.95, 0.954, 0.94, 0.937,\n",
        "            0.947, 0.97, 0.961, 0.936, 0.989, 0.959, 0.932, 0.963, 0.995, 0.959, 0.962]\n",
        "\n",
        "validation_acc = [0.906, 0.949, 0.956, 0.959, 0.939, 0.965, 0.957, 0.959, 0.872, 0.943, 0.954, 0.957, 0.957, 0.952, 0.948, 0.951, 0.94, 0.935,\n",
        "                  0.942, 0.963, 0.959, 0.932, 0.987, 0.956, 0.93, 0.961, 0.995, 0.964, 0.963]\n",
        "\n",
        "transfer_learning = [i for i in range(7)]\n",
        "randomly_init = [13, 15, 17, 19]\n",
        "semi_models = [7, 8, 9, 10, 11, 12, 14, 16, 18, 20]\n",
        "sequence_models = [i for i in range(21,29)]\n",
        "\n",
        "models = [models[i] for i in sequence_models]\n",
        "parameters = [parameters[i] for i in sequence_models]\n",
        "test_acc = [test_acc[i] for i in sequence_models]\n",
        "validation_acc = [validation_acc[i] for i in sequence_models]\n",
        "\n",
        "\n",
        "len(models), len(parameters), len(test_acc)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(parameters, validation_acc, 'ro', label='Validation Results')\n",
        "plt.plot(parameters, test_acc, 'bo', label='Test Results')\n",
        "plt.legend()\n",
        "fig.set_size_inches(15, 15)\n",
        "texts = [plt.text(parameters[i], test_acc[i], models[i]) for i in range(len(test_acc))]\n",
        "#adjust_text(texts)\n",
        "texts = [plt.text(parameters[i], validation_acc[i], models[i]) for i in range(len(validation_acc))]\n",
        "#adjust_text(texts)\n",
        "plt.xlabel('Number of parameters (Millions)', fontweight='bold')\n",
        "plt.ylabel('Accuracy', fontweight='bold')\n",
        "plt.xlim(right=11)\n",
        "plt.grid()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fIcuw9Om1nb"
      },
      "source": [
        "\"\"\"\n",
        "y = test_acc\n",
        "z = parameters\n",
        "n = models\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(z, y)\n",
        "fig.set_size_inches(18.5, 10.5)\n",
        "\n",
        "for i, txt in enumerate(n):\n",
        "    ax.annotate(txt, (z[i], y[i]))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq7tXUBbCK8r"
      },
      "source": [
        "# Google Drive mount\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnN842VcCM_X"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grRid-_2CTi5"
      },
      "source": [
        "# Copying the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zbzju-llCUoO"
      },
      "source": [
        "!cp /content/drive/My\\ Drive/NLP_Nwishy/dataset/new_copy_of_english_dataset_zipped.zip /root/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xKo0Q-sCYMR"
      },
      "source": [
        "%cd /root/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nvOF4IGCaQA"
      },
      "source": [
        "!unzip new_copy_of_english_dataset_zipped.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNR91PuiCcr7"
      },
      "source": [
        "!rm new_copy_of_english_dataset_zipped.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk3HAK9oCepc"
      },
      "source": [
        "%cd /root/english_dataset/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MD7gTnUyyI3"
      },
      "source": [
        "#!rm -r train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSwyqdVPw3ql"
      },
      "source": [
        "import os\n",
        "os.listdir('.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytI9wYFfo-kK"
      },
      "source": [
        "distribution = {}\n",
        "for dataset in os.listdir('.'):\n",
        "  distribution[dataset] = {}\n",
        "  for label in os.listdir(f'{dataset}'):\n",
        "    if label in ['unknown', 'silence']:\n",
        "      continue\n",
        "    distribution[dataset][label] = len(os.listdir(f\"{dataset}/{label}\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ3BOxGHqME5"
      },
      "source": [
        "distribution"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-C0Tkw6ZlmJ"
      },
      "source": [
        "yvals = []\n",
        "for item in distribution['train']:\n",
        "  yvals.append(distribution['train'][item])\n",
        "\n",
        "zvals = []\n",
        "for item in distribution['validation']:\n",
        "  zvals.append(distribution['validation'][item])\n",
        "\n",
        "kvals = []\n",
        "for item in distribution['test']:\n",
        "  kvals.append(distribution['test'][item])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqsV-XlNadLG"
      },
      "source": [
        "yvals, zvals, kvals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2A45omSfr1d"
      },
      "source": [
        "yvals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojwJIKVfbKS6"
      },
      "source": [
        "items = [item for item in distribution['test']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTAZ-Qr8bSmn"
      },
      "source": [
        "items"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBE8DuZcYQ6n"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# set width of bar\n",
        "barWidth = 0.25\n",
        " \n",
        "# set height of bar\n",
        "bars1 = yvals\n",
        "bars2 = zvals\n",
        "bars3 = kvals\n",
        " \n",
        "# Set position of bar on X axis\n",
        "r1 = np.arange(len(bars1))\n",
        "r2 = [x + barWidth for x in r1]\n",
        "r3 = [x + barWidth for x in r2]\n",
        " \n",
        "# Make the plot\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.bar(r1, bars1, color='#7f6d5f', width=barWidth, edgecolor='white', label='train')\n",
        "plt.bar(r2, bars2, color='#557f2d', width=barWidth, edgecolor='white', label='validation')\n",
        "plt.bar(r3, bars3, color='#2d7f5e', width=barWidth, edgecolor='white', label='test')\n",
        " \n",
        "# Add xticks on the middle of the group bars\n",
        "plt.xlabel('Labels', fontweight='bold')\n",
        "plt.ylabel('Number of samples', fontweight='bold')\n",
        "\n",
        "plt.xticks([r + barWidth for r in range(len(bars1))], items)\n",
        " \n",
        "# Create legend & Show graphic\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXv4C1VRo5ON"
      },
      "source": [
        "yvals, zvals, kvals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1clcO_Qbo7qO"
      },
      "source": [
        "np.sum(yvals) , np.sum(zvals) , np.sum(kvals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZuNJA0YpkR6"
      },
      "source": [
        "150000/173682"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu6gwuxXooIm"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pie chart, where the slices will be ordered and plotted counter-clockwise:\n",
        "labels = 'Train', 'Validation', 'Test', 'Submission (unlabeled)'\n",
        "sizes = [np.sum(yvals)*100/173682, np.sum(zvals)*100/173682, np.sum(kvals)*100/173682, 150000*100/173682]\n",
        "explode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
        "\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOU9XKLry4Ju"
      },
      "source": [
        "# **Getting GAN Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJ2m1eMb0tzg"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wybqkpjy8ZP"
      },
      "source": [
        "%cd /root/english_dataset/\n",
        "!mkdir train\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlxufiTZ0W5I"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Dataset/dataset_gan\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzKexusc0_Yq"
      },
      "source": [
        "!cp -r /content/drive/My\\ Drive/NLP_Dataset/dataset_gan/. /root/english_dataset/train/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81jIhwl62XGX"
      },
      "source": [
        "%cd /root/english_dataset/train/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWzC6rAe3aMo"
      },
      "source": [
        "!unzip '*.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szqRzgxl3-mM"
      },
      "source": [
        "!find /root/english_dataset/train -type f -iname '*.zip' -delete"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqVg3gtpw39p"
      },
      "source": [
        "# Copying the dataset(submission)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFV02ekUw39s"
      },
      "source": [
        "!cp /content/drive/My\\ Drive/NLP_Dataset/english_dataset/test_zipped.zip /root/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebxsUmmMw39z"
      },
      "source": [
        "%cd /root/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02VWAHYcw394"
      },
      "source": [
        "!unzip /root/test_zipped.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGhTDtXCw39-"
      },
      "source": [
        "!rm /root/test_zipped.zip   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrWj81juw3-F"
      },
      "source": [
        "%cd /root/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0tu2X9-Co1o"
      },
      "source": [
        "# Background noise generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4smCnWcCqLG"
      },
      "source": [
        "from scipy.io import wavfile\n",
        "import os\n",
        "import numpy as np  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Exmdlqb1CsIO"
      },
      "source": [
        "%cd /root/english_dataset/train/silence/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uczbHE5aCt6Q"
      },
      "source": [
        "noise = []\n",
        "waves = [f for f in os.listdir('./') if f.endswith('.wav')]\n",
        "for wav in waves:\n",
        "  fs,samples = wavfile.read(wav)\n",
        "  noise.append(samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNcv8oFFCwb8"
      },
      "source": [
        "%cd /root/english_dataset/validation/silence/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9A-oPJu4Cx7e"
      },
      "source": [
        "waves = [f for f in os.listdir('./') if f.endswith('.wav')]\n",
        "for wav in waves:\n",
        "  fs,samples = wavfile.read(wav)\n",
        "  noise.append(samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjrf069kC0p_"
      },
      "source": [
        "%cd /root/english_dataset/test/silence/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQGS659ZC2IA"
      },
      "source": [
        "waves = [f for f in os.listdir('./') if f.endswith('.wav')]\n",
        "for wav in waves:\n",
        "  fs,samples = wavfile.read(wav)\n",
        "  noise.append(samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw514dOdC4EB"
      },
      "source": [
        "noise = np.asarray(noise)\n",
        "noise.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgtH5fVKDOuK"
      },
      "source": [
        "# transforms_wav"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ihybnfQDP4N"
      },
      "source": [
        "import random\n",
        "import math    \n",
        "import numpy as np\n",
        "from scipy.io import wavfile\n",
        "from scipy.fftpack import fft,ifft\n",
        "from scipy.signal import resample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAVxtsS8DVPX"
      },
      "source": [
        "def should_apply_transform(prob=0.5):\n",
        "    \"\"\"Transforms are only randomly applied with the given probability.\"\"\"\n",
        "    return random.random() < prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEc8FfQcDX7A"
      },
      "source": [
        "def LoadAudio(path,sample_rate=16000):\n",
        "    \"\"\"Loads an audio into a numpy array.\"\"\"\n",
        "    sample_rate, data = wavfile.read(path, sample_rate)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEnx-PJLGB-z"
      },
      "source": [
        "def FixAudioLength(data,time = 1,sample_rate = 16000):\n",
        "    \"\"\"fixes audio length to be 16000 sample.\"\"\"\n",
        "    length = int(time * sample_rate)\n",
        "    if length < len(data):\n",
        "        data = data[:length]\n",
        "    elif length > len(data):\n",
        "        data = np.pad(data, (0, length - len(data)), \"constant\")\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSMneZEmDs65"
      },
      "source": [
        "def change_volume(data,factor_1=random.uniform(0.2,2),factor_2=random.uniform(0.2,2),zone_1=random.uniform(0,0.49),zone_2=random.uniform(0,0.5)):\n",
        "    if not should_apply_transform():\n",
        "      return data\n",
        "    \"\"\"Changes audio volume.\"\"\"\n",
        "    data_s1 = math.ceil(zone_1 * len(data))  ##get the size of the first zone\n",
        "    data_s2 = math.ceil(zone_2 * len(data))  ##get the size of the second zone\n",
        "    if data_s1 < data_s2 :\n",
        "        data_1 = data[0:data_s1] * factor_1 ##increase the volume (first zone) by factor_1\n",
        "        data_2 = data[data_s1:data_s2]\n",
        "        data_3 = data[data_s2:] * factor_2  ##increase the volume (second zone) by factor_2\n",
        "    if data_s1 >= data_s2 :\n",
        "        data_1 = data[0:data_s2] * factor_1\n",
        "        data_2 = data[data_s2:data_s1]\n",
        "        data_3 = data[data_s1:] * factor_2\n",
        "    data = np.concatenate((data_1,data_2,data_3))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88J4tVosHkTE"
      },
      "source": [
        "def mask(data,mask_with_noise=False,zone_1=random.uniform(0,0.49),zone_2=random.uniform(0,0.5)):\n",
        "    if not should_apply_transform():\n",
        "      return data\n",
        "    \"\"\"maskes an audio in time domain.\"\"\"\n",
        "    data_s1 = math.ceil(zone_1 * len(data)) ##get the size of the first zone\n",
        "    data_s2 = math.ceil(zone_2 * len(data)) ##get the size of the second zone\n",
        "    if mask_with_noise == True :\n",
        "        if data_s1 < data_s2 :\n",
        "            data_1 = data[0:data_s1] \n",
        "            data_2 = 100*np.random.normal(0,1,data_s2-data_s1)  ##if mask_with_noise == True replace the zone by noise\n",
        "            data_3 = data[data_s2:] \n",
        "        if data_s1 >= data_s2 :\n",
        "            data_1 = data[0:data_s2] \n",
        "            data_2 = 100*np.random.normal(0,1,data_s1-data_s2)\n",
        "            data_3 = data[data_s1:]\n",
        "    if mask_with_noise == False :\n",
        "        if data_s1 < data_s2 :\n",
        "            data_1 = data[0:data_s1] \n",
        "            data_2 = np.zeros(data_s2-data_s1)\n",
        "            data_3 = data[data_s2:] \n",
        "        if data_s1 >= data_s2 :\n",
        "            data_1 = data[0:data_s2] \n",
        "            data_2 = np.zeros(data_s1-data_s2)  ##if mask_with_noise == False replace the noise by zeros\n",
        "            data_3 = data[data_s1:]\n",
        "    data = np.concatenate((data_1,data_2,data_3))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SeoDqdxIRvl"
      },
      "source": [
        "def speed_change(data,factor_1=random.uniform(0.2,2),factor_2=random.uniform(0.2,2),zone_1=random.uniform(0,0.49),zone_2=random.uniform(0,0.5)):\n",
        "    if not should_apply_transform():\n",
        "      return data\n",
        "    \"\"\"changes audio speed.\"\"\"\n",
        "    def speedx(sound_array, factor):\n",
        "        ##speed sound by factor\n",
        "        indices = np.round( np.arange(0, len(sound_array), factor) )\n",
        "        indices = indices[indices < len(sound_array)].astype(int)\n",
        "        return sound_array[indices.astype(int)]\n",
        "        \n",
        "    data_s1 = math.ceil(zone_1 * len(data)) ##get the size of the first zone\n",
        "    data_s2 = math.ceil(zone_2 * len(data)) ##get the size of the second zone\n",
        "    if data_s1 < data_s2 :\n",
        "        data_1 = speedx(data[0:data_s1], factor_1)  ##speed the first zone by factor_1\n",
        "        data_2 = data[data_s1:data_s2]\n",
        "        data_3 = speedx(data[data_s2:] , factor_2)  ##speed the third zone by factor_2\n",
        "    if data_s1 >= data_s2 :\n",
        "        data_1 = speedx(data[0:data_s2] , factor_1)\n",
        "        data_2 = data[data_s2:data_s1]\n",
        "        data_3 = speedx(data[data_s1:], factor_2)\n",
        "    audio_f = np.concatenate((data_1,data_2,data_3))  ##concatenate the 3 zones\n",
        "    audio_f = resample(audio_f,data.shape[0])\n",
        "    data = audio_f.astype(type(data[0]))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Th6p95EWJh_F"
      },
      "source": [
        "def pitch_shift(data,sampling_rate=16000,window_size=2**8,overlap_factor=random.randint(25,80)/100,n_of_semitones=random.randint(-10,10)):  \n",
        "    if not should_apply_transform():\n",
        "      return data\n",
        "    \"\"\"changes audio pitch.\"\"\"      \n",
        "    def createFrames(audio,hop,window_size):\n",
        "        length =len(audio)\n",
        "        n_slices = math.floor((length - window_size) / hop)\n",
        "        audio = audio[0:(n_slices*hop+window_size)]\n",
        "        v_frames = np.zeros((math.floor(length/hop),window_size)) \n",
        "        for index in range (0,n_slices):\n",
        "             index_time_start = (index)*hop +1\n",
        "             index_time_end = (index)*hop + window_size\n",
        "             v_frames[index][:] = audio[index_time_start-1 : index_time_end]\n",
        "        return v_frames , n_slices \n",
        "        \n",
        "    def fusionFrames(frames_matrix, hop):\n",
        "        numberFrames = frames_matrix.shape[0]\n",
        "        sizeFrames = frames_matrix.shape[1]\n",
        "        vectorTime = np.zeros(numberFrames*hop-hop+sizeFrames)\n",
        "        timeIndex = 0\n",
        "        for index in range(0,numberFrames):\n",
        "             vectorTime[timeIndex:timeIndex+sizeFrames] = vectorTime[timeIndex:timeIndex+sizeFrames] + np.transpose(frames_matrix[index][:])\n",
        "             timeIndex = timeIndex + hop\n",
        "        return vectorTime\n",
        "        \n",
        "    if n_of_semitones != 0:\n",
        "        win_size = window_size\n",
        "        hop = int(win_size*overlap_factor)\n",
        "        alpha = 2**(n_of_semitones/12)\n",
        "        hop_out = round(alpha*hop)\n",
        "        window_arr = np.blackman(win_size*2+1)\n",
        "        window_arr = window_arr[2::2]\n",
        "        original_audio = data\n",
        "        original_audio_new = np.hstack(((np.zeros(hop*3)),original_audio))\n",
        "        framed_audio,number_frames_in = createFrames(original_audio_new,hop,win_size)\n",
        "        number_frames_out = number_frames_in\n",
        "        out = np.zeros((number_frames_out,win_size))\n",
        "        phase_cumulative = np.zeros((1,framed_audio.shape[1]))\n",
        "        previous_phase = np.zeros((1,framed_audio.shape[1]))\n",
        "\n",
        "        for index in range (0,number_frames_in):\n",
        "            current_frame = framed_audio[index][:]\n",
        "            current_frame_windowed = current_frame* np.transpose(window_arr) / math.sqrt(((win_size/hop)/2))\n",
        "            current_frame_windowed_FFT = fft(current_frame_windowed)\n",
        "            mag_frame = abs(current_frame_windowed_FFT)\n",
        "            phase_frame = np.angle(current_frame_windowed_FFT)\n",
        "            deltaPhi = phase_frame - previous_phase\n",
        "            previous_phase = phase_frame\n",
        "            deltaPhiPrime = deltaPhi - hop * 2*math.pi*np.arange(0,win_size)/win_size\n",
        "            deltaPhiPrimeMod = ((deltaPhiPrime+math.pi) % (2*math.pi)) - (math.pi)\n",
        "            trueFreq = (2*math.pi*np.arange(0,win_size)/win_size) + (deltaPhiPrimeMod/hop)\n",
        "            phase_cumulative = phase_cumulative + hop_out * trueFreq\n",
        "            outputMag = mag_frame\n",
        "            outputFrame = np.real(ifft(outputMag * np.exp(1j*phase_cumulative)))\n",
        "            out[index][:] = (outputFrame * np.transpose(window_arr)) / math.sqrt(((win_size/hop_out)/2))\n",
        "    \n",
        "        outputTimeStretched = fusionFrames(out,hop_out)\n",
        "        outputTime = resample(outputTimeStretched,int(len(outputTimeStretched)/alpha))\n",
        "        outputVector = resample(outputTime,sampling_rate)\n",
        "        data = outputVector.astype(type(data[0]))   \n",
        "        return data\n",
        "        \n",
        "    else:\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTfECCFXLXUH"
      },
      "source": [
        "def fourier_transform(data,sampling_rate=16000,shift_factor=random.randint(-1000,1000)):\n",
        "    if not should_apply_transform():\n",
        "      return data\n",
        "    \"\"\"shifts audio in frequency domain.\"\"\"\n",
        "    audio_FFT = fft(data)\n",
        "    rolled_audio_FFT = np.roll(audio_FFT,shift_factor)\n",
        "    if shift_factor > 0:\n",
        "        rolled_audio_FFT = np.hstack((np.zeros(shift_factor),rolled_audio_FFT[shift_factor:]))\n",
        "    else:\n",
        "        rolled_audio_FFT = np.hstack((rolled_audio_FFT[:len(rolled_audio_FFT)+shift_factor],np.zeros(-shift_factor)))\n",
        "        \n",
        "    shifted_audio_IFFT = ifft(rolled_audio_FFT) \n",
        "    data = shifted_audio_IFFT.astype(type(data[0]))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiZCNglzL93w"
      },
      "source": [
        "def crop_audio(data,sampling_rate=16000,start=random.uniform(0,0.3),stop=random.uniform(0.7,1),duration_in_sec=1):\n",
        "    if not should_apply_transform():\n",
        "      return data\n",
        "    start = int(start*duration_in_sec*sampling_rate)\n",
        "    stop = int(stop*duration_in_sec*sampling_rate)\n",
        "    cropped_audio = data[start:stop]\n",
        "    cropped_audio = resample(cropped_audio,sampling_rate)\n",
        "    data = cropped_audio.astype(type(data[0]))\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eVxvTThML5w"
      },
      "source": [
        "class AddBackgroundNoise():\n",
        "    \"\"\"Adds a random background noise.\"\"\"\n",
        "    def __init__(self, bg_dataset, max_percentage=0.45):\n",
        "        self.bg_dataset = bg_dataset\n",
        "        self.max_percentage = max_percentage\n",
        "\n",
        "    def augment(self, data):\n",
        "        if not should_apply_transform():\n",
        "          return data\n",
        "        noise = random.choice(self.bg_dataset)\n",
        "        #data = FixAudioLength(data)\n",
        "        percentage = random.uniform(0, self.max_percentage)\n",
        "        data = data * (1 - percentage) + noise * percentage\n",
        "        return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ceqpd7FpWXyD"
      },
      "source": [
        "inject_noise = AddBackgroundNoise(noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG0p5wiRNCGQ"
      },
      "source": [
        "def time_shift(data,sampling_rate=16000,direction='both',max_shift=300):\n",
        "    if not should_apply_transform():\n",
        "      return data\n",
        "    coverage_maxlimit = (max_shift/1000) * sampling_rate\n",
        "    start = int(random.randint(0,coverage_maxlimit))\n",
        "    if direction == 'right':\n",
        "        start = -start\n",
        "    elif direction == 'both':\n",
        "        rand_direction = np.random.randint(0, 2)\n",
        "        '''\n",
        "        0: left direction\n",
        "        1: right direction\n",
        "        '''\n",
        "        if rand_direction == 1:\n",
        "            start = -start\n",
        "                \n",
        "    if start >= 0:\n",
        "        data = np.r_[data[start:], np.random.uniform(-0.001,0.001, start)]\n",
        "    else:\n",
        "        data = np.r_[np.random.uniform(-0.001,0.001, -start), data[:start]]\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJVbFnxGWmMc"
      },
      "source": [
        "# Features transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "305qrCfIWnEy"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UY4Y5qrmXjJ6"
      },
      "source": [
        "pip install python_speech_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oLvwCFcXkGa"
      },
      "source": [
        "from python_speech_features import mfcc as mfcc_features\n",
        "from python_speech_features import logfbank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJhSEKOQWo3X"
      },
      "source": [
        "def ToTensor(data):\n",
        "  \"\"\"Converts into a tensor.\"\"\"\n",
        "  tensor = torch.FloatTensor(data)\n",
        "  return tensor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXdpqxVZWqYy"
      },
      "source": [
        "#def ToMFCC(data):\n",
        "    #mfcc=logfbank(data,16000,nfilt=32,winstep=0.032)\n",
        "    #mfcc = np.asarray(mfcc)\n",
        "    #mfcc = ToTensor(mfcc)\n",
        "    #return mfcc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USrD2ksswiBZ"
      },
      "source": [
        "import librosa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMDNynT8wXjE"
      },
      "source": [
        "def ToSTFT(data,n_fft=2048, hop_length=512):\n",
        "  \"\"\"Applies on an audio the short time fourier transform.\"\"\"\n",
        "  data_stft = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)\n",
        "  return data_stft  \n",
        "\n",
        "def ToMFCC(data,n_mels=32,sample_rate=16000,n_fft=2048):\n",
        "  \"\"\"Creates the mel spectrogram from the short time fourier transform of a file. The result is a 32x32 matrix.\"\"\"\n",
        "  data = data.astype(np.float32, order='C') / 32768.0\n",
        "  stft = ToSTFT(data)\n",
        "  mel_basis = librosa.filters.mel(sample_rate, n_fft, n_mels)\n",
        "  s = np.dot(mel_basis, np.abs(stft)**2.0)\n",
        "  data_mfcc = librosa.power_to_db(s, ref=np.max)\n",
        "  #data_mfcc = normalization(data_mfcc)\n",
        "  #data_mfcc = ToTensor(data_mfcc)\n",
        "  return data_mfcc  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVYcPF0tWoUF"
      },
      "source": [
        "%cd /root/english_dataset/train/down  \n",
        "from scipy.io import wavfile\n",
        "fs, data = wavfile.read('5718.wav')\n",
        "mfccs = ToMFCC(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A8ZEihNX7Zg"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "import librosa.display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBc6a8zRVYFM"
      },
      "source": [
        "librosa.display.specshow(mfccs, x_axis='time')\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.title('mfcc')\n",
        "plt.show\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzdoYmZmnnAe"
      },
      "source": [
        "def ToSTFT(data,n_fft=2048, hop_length=512):\n",
        "  \"\"\"Applies on an audio the short time fourier transform.\"\"\"\n",
        "  data_stft = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)\n",
        "  return data_stft  \n",
        "\n",
        "def ToMFCC(data,n_mels=32,sample_rate=16000,n_fft=2048):\n",
        "  \"\"\"Creates the mel spectrogram from the short time fourier transform of a file. The result is a 32x32 matrix.\"\"\"\n",
        "  data = data.astype(np.float32, order='C') / 32768.0\n",
        "  ##########################################################################################\n",
        "  #stft = ToSTFT(data)\n",
        "  #mel_basis = librosa.filters.mel(sample_rate, n_fft, n_mels)\n",
        "  #s = np.dot(mel_basis, np.abs(stft)**2.0)\n",
        "  #data_mfcc = librosa.power_to_db(s, ref=np.max)\n",
        "  ######################################################################################\n",
        "  #data_mfcc = normalization(data_mfcc)\n",
        "  #data_mfcc = ToTensor(data_mfcc)\n",
        "  return data    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb3TDBQIYwA1"
      },
      "source": [
        "# dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoLxWzaQYxBN"
      },
      "source": [
        "import sys, os\n",
        "from os.path import isdir, join\n",
        "import pandas as pd\n",
        "from torch import Tensor\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision.transforms import Compose\n",
        "from torch.utils.data.sampler import WeightedRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEXh1nrCY9wp"
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "print('use_gpu', use_gpu)\n",
        "if use_gpu:\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2CTUL9vY_1W"
      },
      "source": [
        "class Freesound(Dataset):\n",
        "    def __init__(self, transform=None, mode=\"train\"):\n",
        "        # setting directories for data\n",
        "        data_root = \"/root/english_dataset\"\n",
        "        self.mode = mode\n",
        "        if self.mode is \"train\":\n",
        "            self.data_dir = os.path.join(data_root, \"train\")\n",
        "            dirs=[]\n",
        "            for f in os.listdir(self.data_dir):\n",
        "              if isdir(join(self.data_dir, f)):\n",
        "                dirs.append(f)\n",
        "            dirs.sort()\n",
        "            all_waves = []\n",
        "            all_labels = []\n",
        "            for direct in dirs:\n",
        "                waves = [f for f in os.listdir(join(self.data_dir, direct)) if f.endswith('.wav')]\n",
        "                waves_temp = [direct + '/' + s for s in waves]\n",
        "                all_waves += waves_temp\n",
        "                labels = list(np.repeat(direct, len(waves)))\n",
        "                all_labels += labels\n",
        "            self.all_waves = all_waves\n",
        "            self.all_labels = all_labels\n",
        "            #self.csv_file = pd.read_csv(os.path.join(data_root, \"train.csv\"))\n",
        "#########################################################validation################################################\n",
        "        elif self.mode is \"validation\":\n",
        "            self.data_dir = os.path.join(data_root, \"validation\")\n",
        "            dirs=[]\n",
        "            for f in os.listdir(self.data_dir):\n",
        "              if isdir(join(self.data_dir, f)):\n",
        "                dirs.append(f)\n",
        "            dirs.sort()\n",
        "            all_waves = []\n",
        "            all_labels = []\n",
        "            for direct in dirs:\n",
        "                waves = [f for f in os.listdir(join(self.data_dir, direct)) if f.endswith('.wav')]\n",
        "                waves_temp = [direct + '/' + s for s in waves]\n",
        "                all_waves += waves_temp\n",
        "                labels = list(np.repeat(direct, len(waves)))\n",
        "                all_labels += labels\n",
        "            self.all_waves = all_waves\n",
        "            self.all_labels = all_labels\n",
        "#####################################################################################################################\n",
        "        elif self.mode is \"test\":\n",
        "            self.data_dir = os.path.join(data_root, \"test\")\n",
        "            dirs=[]\n",
        "            for f in os.listdir(self.data_dir):\n",
        "              if isdir(join(self.data_dir, f)):\n",
        "                dirs.append(f)\n",
        "            dirs.sort()\n",
        "            all_waves = []\n",
        "            all_labels = []\n",
        "            for direct in dirs:\n",
        "                waves = [f for f in os.listdir(join(self.data_dir, direct)) if f.endswith('.wav')]\n",
        "                waves_temp = [direct + '/' + s for s in waves]\n",
        "                all_waves += waves_temp\n",
        "                labels = list(np.repeat(direct, len(waves)))\n",
        "                all_labels += labels\n",
        "            self.all_waves = all_waves\n",
        "            self.all_labels = all_labels\n",
        "#####################################################################################################################\n",
        "        elif self.mode is \"submission\":\n",
        "            self.data_dir = os.path.join('/root', \"test/audio\")\n",
        "            #print(self.data_dir)\n",
        "            all_waves = [f for f in os.listdir(self.data_dir) if f.endswith('.wav')]\n",
        "            #print(len(all_waves))\n",
        "            self.all_waves = all_waves\n",
        "\n",
        "############################################################################################################################\n",
        "        elif self.mode is \"semi_supervised\":\n",
        "            self.data_dir = os.path.join('/root', \"test/audio\")\n",
        "            lineList = [line.rstrip('\\n') for line in open(\"/content/drive/My Drive/NLP_Dataset/submissions/semi_supervised.txt\")]\n",
        "            lineList = lineList[1:]\n",
        "            all_waves = [line.split(',')[0] for line in lineList]   \n",
        "            all_labels = [line.split(',')[1] for line in lineList]     \n",
        "            self.all_waves = all_waves\n",
        "            self.all_labels = all_labels\n",
        "\n",
        "##################################################End Validation##############################################################\n",
        "            #self.data_dir = os.path.join(data_root, \"validation\")\n",
        "            #self.csv_file = pd.read_csv(os.path.join(data_root, \"sample_submission.csv\"))\n",
        "\n",
        "        # dict for mapping class names into indices. can be obtained by \n",
        "        # {cls_name:i for i, cls_name in enumerate(csv_file[\"label\"].unique())}\n",
        "        self.classes=  {'down':0,\n",
        "          'go':1,\n",
        "          'left':2,\n",
        "          'no':3,\n",
        "          'off':4,\n",
        "          'on':5,\n",
        "          'right':6,\n",
        "          'silence':7,\n",
        "          'stop':8,\n",
        "          'unknown':9,\n",
        "          'up':10,\n",
        "          'yes':11,}\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.all_waves) \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.all_waves[idx]\n",
        "         \n",
        "        data = LoadAudio(os.path.join(self.data_dir, filename),sample_rate=16000)\n",
        "        #rate, data = wavfile.read(os.path.join(self.data_dir, filename))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        if self.mode is \"train\":\n",
        "            label = self.classes[self.all_labels[idx]]\n",
        "            return data, label\n",
        "\n",
        "        elif (self.mode is \"validation\") or (self.mode is \"test\") or (self.mode is \"semi_supervised\"):\n",
        "            label = self.classes[self.all_labels[idx]]\n",
        "            return data, label\n",
        "\n",
        "        elif (self.mode is \"submission\"):\n",
        "            return data, filename\n",
        "\n",
        "    def make_weights_for_balanced_classes(self):\n",
        "      \"\"\"adopted from https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\"\"\"\n",
        "      nclasses = len(self.classes)\n",
        "      count = np.zeros(nclasses)\n",
        "      for item in self.all_labels:\n",
        "          count[self.classes[item]] += 1\n",
        "\n",
        "      N = float(sum(count))\n",
        "      weight_per_class = N / count\n",
        "      weight = np.zeros(len(self))\n",
        "      for idx in range(len(self)):\n",
        "          weight[idx] = weight_per_class[self.classes[self.all_labels[idx]]]\n",
        "      return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj6sv7zU6XsH"
      },
      "source": [
        "class Freesound(Dataset):\n",
        "    def __init__(self, transform=None, mode=\"train\"):\n",
        "        # setting directories for data\n",
        "        data_root = \"/root/english_dataset\"\n",
        "        self.mode = mode\n",
        "        if self.mode is \"train\":\n",
        "            self.data_dir = os.path.join(data_root, \"train\")\n",
        "            dirs=[]\n",
        "            for f in os.listdir(self.data_dir):\n",
        "              if isdir(join(self.data_dir, f)):\n",
        "                dirs.append(f)\n",
        "            dirs.sort()\n",
        "            all_waves = []\n",
        "            all_labels = []\n",
        "            for direct in dirs:\n",
        "                if direct in ['unknown', 'silence']:\n",
        "                  waves = [f for f in os.listdir(join(self.data_dir, direct)) if f.endswith('.wav')]\n",
        "                  waves_temp = [direct + '/' + s for s in waves]\n",
        "                  all_waves += waves_temp\n",
        "                  labels = list(np.repeat(direct, len(waves)))\n",
        "                  all_labels += labels\n",
        "                else:\n",
        "                  folders = [f for f in os.listdir(join(self.data_dir, direct))]\n",
        "                  for folder in folders:\n",
        "                    waves = [f for f in os.listdir(f\"{self.data_dir}/{direct}/{folder}\") if f.endswith('.wav')]\n",
        "                    waves_temp = [direct + '/' + folder + '/' + s for s in waves]\n",
        "                    all_waves += waves_temp\n",
        "                    labels = list(np.repeat(folder, len(waves)))\n",
        "                    all_labels += labels\n",
        "                  \n",
        "            self.all_waves = all_waves\n",
        "            self.all_labels = all_labels\n",
        "            #self.csv_file = pd.read_csv(os.path.join(data_root, \"train.csv\"))\n",
        "#########################################################validation################################################\n",
        "        elif self.mode is \"validation\":\n",
        "            self.data_dir = os.path.join(data_root, \"validation\")\n",
        "            dirs=[]\n",
        "            for f in os.listdir(self.data_dir):\n",
        "              if isdir(join(self.data_dir, f)):\n",
        "                dirs.append(f)\n",
        "            dirs.sort()\n",
        "            all_waves = []\n",
        "            all_labels = []\n",
        "            for direct in dirs:\n",
        "                waves = [f for f in os.listdir(join(self.data_dir, direct)) if f.endswith('.wav')]\n",
        "                waves_temp = [direct + '/' + s for s in waves]\n",
        "                all_waves += waves_temp\n",
        "                labels = list(np.repeat(direct, len(waves)))\n",
        "                all_labels += labels\n",
        "            self.all_waves = all_waves\n",
        "            self.all_labels = all_labels\n",
        "#####################################################################################################################\n",
        "        elif self.mode is \"test\":\n",
        "            self.data_dir = os.path.join(data_root, \"test\")\n",
        "            dirs=[]\n",
        "            for f in os.listdir(self.data_dir):\n",
        "              if isdir(join(self.data_dir, f)):\n",
        "                dirs.append(f)\n",
        "            dirs.sort()\n",
        "            all_waves = []\n",
        "            all_labels = []\n",
        "            for direct in dirs:\n",
        "                waves = [f for f in os.listdir(join(self.data_dir, direct)) if f.endswith('.wav')]\n",
        "                waves_temp = [direct + '/' + s for s in waves]\n",
        "                all_waves += waves_temp\n",
        "                labels = list(np.repeat(direct, len(waves)))\n",
        "                all_labels += labels\n",
        "            self.all_waves = all_waves\n",
        "            self.all_labels = all_labels\n",
        "#####################################################################################################################\n",
        "        elif self.mode is \"submission\":\n",
        "            self.data_dir = os.path.join('/root', \"test/audio\")\n",
        "            #print(self.data_dir)\n",
        "            all_waves = [f for f in os.listdir(self.data_dir) if f.endswith('.wav')]\n",
        "            #print(len(all_waves))\n",
        "            self.all_waves = all_waves\n",
        "\n",
        "############################################################################################################################\n",
        "        elif self.mode is \"semi_supervised\":\n",
        "            self.data_dir = os.path.join('/root', \"test/audio\")\n",
        "            lineList = [line.rstrip('\\n') for line in open(\"/content/drive/My Drive/NLP_Dataset/submissions/semi_supervised.txt\")]\n",
        "            lineList = lineList[1:]\n",
        "            all_waves = [line.split(',')[0] for line in lineList]   \n",
        "            all_labels = [line.split(',')[1] for line in lineList]     \n",
        "            self.all_waves = all_waves\n",
        "            self.all_labels = all_labels\n",
        "\n",
        "##################################################End Validation##############################################################\n",
        "            #self.data_dir = os.path.join(data_root, \"validation\")\n",
        "            #self.csv_file = pd.read_csv(os.path.join(data_root, \"sample_submission.csv\"))\n",
        "\n",
        "        # dict for mapping class names into indices. can be obtained by \n",
        "        # {cls_name:i for i, cls_name in enumerate(csv_file[\"label\"].unique())}\n",
        "        self.classes=  {'down':0,\n",
        "          'go':1,\n",
        "          'left':2,\n",
        "          'no':3,\n",
        "          'off':4,\n",
        "          'on':5,\n",
        "          'right':6,\n",
        "          'silence':7,\n",
        "          'stop':8,\n",
        "          'unknown':9,\n",
        "          'up':10,\n",
        "          'yes':11,}\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.all_waves) \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename = self.all_waves[idx]\n",
        "         \n",
        "        data = LoadAudio(os.path.join(self.data_dir, filename),sample_rate=16000)\n",
        "        #rate, data = wavfile.read(os.path.join(self.data_dir, filename))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        if self.mode is \"train\":\n",
        "            label = self.classes[self.all_labels[idx]]\n",
        "            return data, label\n",
        "\n",
        "        elif (self.mode is \"validation\") or (self.mode is \"test\") or (self.mode is \"semi_supervised\"):\n",
        "            label = self.classes[self.all_labels[idx]]\n",
        "            return data, label\n",
        "\n",
        "        elif (self.mode is \"submission\"):\n",
        "            return data, filename\n",
        "\n",
        "    def make_weights_for_balanced_classes(self):\n",
        "      \"\"\"adopted from https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3\"\"\"\n",
        "      nclasses = len(self.classes)\n",
        "      count = np.zeros(nclasses)\n",
        "      for item in self.all_labels:\n",
        "          count[self.classes[item]] += 1\n",
        "\n",
        "      N = float(sum(count))\n",
        "      weight_per_class = N / count\n",
        "      weight = np.zeros(len(self))\n",
        "      for idx in range(len(self)):\n",
        "          weight[idx] = weight_per_class[self.classes[self.all_labels[idx]]]\n",
        "      return weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFmijnBYZFyn"
      },
      "source": [
        "data_aug_transform = Compose([lambda x:change_volume(x),\n",
        "                              lambda x:mask(x),\n",
        "                              lambda x:speed_change(x),\n",
        "                              lambda x:pitch_shift(x),\n",
        "                              lambda x:fourier_transform(x),\n",
        "                              lambda x:crop_audio(x),\n",
        "                              lambda x:time_shift(x),\n",
        "                              lambda x:FixAudioLength(x),\n",
        "                              lambda x:inject_noise.augment(x)])\n",
        "train_feature_transform = Compose([lambda x:ToMFCC(x)])\n",
        "train_dataset = Freesound(Compose([data_aug_transform,train_feature_transform]), mode=\"train\")\n",
        "#train_dataset = Freesound(Compose([train_feature_transform]), mode=\"train\")\n",
        "\n",
        "\n",
        "\n",
        "valid_feature_transform = Compose([lambda x:ToMFCC(x)])\n",
        "valid_dataset = Freesound(Compose([lambda x:FixAudioLength(x),\n",
        "                                   lambda x:valid_feature_transform(x)]), mode=\"validation\")\n",
        "\n",
        "\n",
        "test_feature_transform = Compose([lambda x:ToMFCC(x)])\n",
        "test_dataset = Freesound(Compose([lambda x:FixAudioLength(x),\n",
        "                                   lambda x:test_feature_transform(x)]), mode=\"test\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lG72Zumm5QiP"
      },
      "source": [
        "submission_feature_transform = Compose([lambda x:ToMFCC(x)])\n",
        "submission_dataset = Freesound(Compose([lambda x:FixAudioLength(x),\n",
        "                                   lambda x:submission_feature_transform(x)]), mode=\"submission\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o95oy2xAFj6F"
      },
      "source": [
        "semi_supervised_feature_transform = Compose([lambda x:ToMFCC(x)])\n",
        "semi_supervised_dataset = Freesound(Compose([lambda x:FixAudioLength(x),\n",
        "                                   lambda x:semi_supervised_feature_transform(x)]), mode=\"semi_supervised\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoQ0c3yqZM-V"
      },
      "source": [
        "weights = train_dataset.make_weights_for_balanced_classes()\n",
        "sampler = WeightedRandomSampler(weights, len(weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QVTUL-GZOAc"
      },
      "source": [
        "trainloader = DataLoader(train_dataset, batch_size=64,shuffle=False,sampler=sampler,\n",
        "                              pin_memory=use_gpu, num_workers=0)\n",
        "validationloader = DataLoader(valid_dataset, batch_size=64, shuffle=False,\n",
        "                              pin_memory=use_gpu, num_workers=0)  \n",
        "\n",
        "testloader = DataLoader(test_dataset, batch_size=64, shuffle=False,\n",
        "                              pin_memory=use_gpu, num_workers=0) \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wGduhdoB-DO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7AkYBP25Tye"
      },
      "source": [
        "submissionloader = DataLoader(submission_dataset, batch_size=64, shuffle=False,\n",
        "                              pin_memory=use_gpu, num_workers=0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM5XcTQGFzXX"
      },
      "source": [
        "semi_supervisedloader =  DataLoader(semi_supervised_dataset, batch_size=64, shuffle=True,\n",
        "                              pin_memory=use_gpu, num_workers=0)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sajY_Fwdn0dS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUo86lGbn1jI"
      },
      "source": [
        "# **RNN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH6WIKLM-mjf"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Dataset/transfer_learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ2Y0xCq_dDe"
      },
      "source": [
        "MODEL_NAME = 'attRNNhybird'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKwoadR_rlD3"
      },
      "source": [
        "**Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfhgkO4xrn2J"
      },
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras import layers as L\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "\n",
        "from kapre.time_frequency import Melspectrogram, Spectrogram\n",
        "from kapre.utils import Normalization2D\n",
        "import math\n",
        "from tqdm.notebook import tqdm\n",
        "from keras import backend as K\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wQiz9TvLxCl"
      },
      "source": [
        "import keras\n",
        "keras.__version__\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYHIMpCo9tZz"
      },
      "source": [
        "def AttRNNSpeechModel(nCategories, samplingrate=16000,\n",
        "                      inputLength=16000, rnn_func=L.LSTM):\n",
        "    # simple LSTM\n",
        "    sr = samplingrate\n",
        "    iLen = inputLength\n",
        "\n",
        "    inputs = L.Input((inputLength,), name='input')\n",
        "\n",
        "    x = L.Reshape((1, -1))(inputs)\n",
        "\n",
        "    m = Melspectrogram(n_dft=1024, n_hop=512, input_shape=(1, iLen),\n",
        "                       padding='same', sr=sr, n_mels=32,\n",
        "                       fmin=40.0, fmax=sr / 2, power_melgram=1.0,\n",
        "                       return_decibel_melgram=True, trainable_fb=False,\n",
        "                       trainable_kernel=False,\n",
        "                       name='mel_stft')\n",
        "    m.trainable = False\n",
        "\n",
        "    x = m(x)\n",
        "\n",
        "    x = Normalization2D(int_axis=0, name='mel_stft_norm')(x)\n",
        "\n",
        "    # note that Melspectrogram puts the sequence in shape (batch_size, melDim, timeSteps, 1)\n",
        "    # we would rather have it the other way around for LSTMs\n",
        "\n",
        "    x = L.Permute((2, 1, 3))(x)\n",
        "\n",
        "    x = L.Conv2D(10, (5, 1), activation='relu', padding='same')(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    x = L.Conv2D(1, (5, 1), activation='relu', padding='same')(x)\n",
        "    x = L.BatchNormalization()(x)\n",
        "    #conv1 = x\n",
        "\n",
        "    # x = Reshape((125, 80)) (x)\n",
        "    # keras.backend.squeeze(x, axis)\n",
        "\n",
        "    #x = K.expand_dims(x)\n",
        "    #conv1 = x\n",
        "    x = L.Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim')(x)\n",
        "\n",
        "    x = L.Bidirectional(rnn_func(64, return_sequences=True)\n",
        "                        )(x)  # [b_s, seq_len, vec_dim]\n",
        "\n",
        "    first_lstm = x\n",
        "    x = L.Bidirectional(rnn_func(64, return_sequences=True)\n",
        "                        )(x)  # [b_s, seq_len, vec_dim]\n",
        "\n",
        "    x = L.Add()([first_lstm, x]) \n",
        "\n",
        "    xFirst = L.Lambda(lambda q: q[:,-1])(x)  # [b_s, vec_dim]\n",
        "    query = L.Dense(128)(xFirst)\n",
        "\n",
        "    # dot product attention\n",
        "    attScores = L.Dot(axes=[1, 2])([query, x])\n",
        "    attScores = L.Softmax(name='attSoftmax')(attScores)  # [b_s, seq_len]\n",
        "\n",
        "    # rescale sequence\n",
        "    attVector = L.Dot(axes=[1, 1])([attScores, x])  # [b_s, vec_dim]\n",
        "\n",
        "    x = L.Dense(64, activation='relu')(attVector)\n",
        "    x = L.Dense(32)(x)\n",
        "\n",
        "    output = L.Dense(nCategories, activation='softmax', name='output')(x)\n",
        "\n",
        "    model = Model(inputs=[inputs], outputs=[output])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0Ttq1zSsCNn"
      },
      "source": [
        "model = AttRNNSpeechModel(nCategories=12)#, rnn_func=L.LSTM)\n",
        "model.compile(optimizer='adam', loss=['sparse_categorical_crossentropy'], metrics=['sparse_categorical_accuracy'])\n",
        "model.summary()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U6zCVjH9wz3"
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(f'{MODEL_NAME}_bestloss.h5', custom_objects={'Melspectrogram': Melspectrogram, 'Normalization2D': Normalization2D })\n",
        "#model.summary()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA99AgbzJFcD"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGdsGcd_vTgc"
      },
      "source": [
        "def step_decay(epoch):\n",
        "    initial_lrate = 0.001\n",
        "    drop = 0.4  \n",
        "    epochs_drop = 15.0\n",
        "    lrate = initial_lrate * math.pow(drop,  \n",
        "            math.floor((1+epoch)/epochs_drop))\n",
        "    \n",
        "    if (lrate < 4e-5):\n",
        "        lrate = 4e-5\n",
        "      \n",
        "    print('Changing learning rate to {}'.format(lrate))\n",
        "    return lrate\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lvfjMwS0SZA"
      },
      "source": [
        "pbar = tqdm(validationloader, unit=\"audios\", unit_scale=validationloader.batch_size)\n",
        "size = validationloader.__len__()\n",
        "loss = 0\n",
        "acc = 0\n",
        "for batch in pbar:\n",
        "    inputs = batch[0]\n",
        "    targets = batch[1]\n",
        "    inputs = inputs.numpy()\n",
        "    targets = targets.numpy()\n",
        "    metric = model.evaluate(inputs,targets,verbose=0)\n",
        "    loss += metric[0]\n",
        "    acc += metric[1]\n",
        "\n",
        "loss = loss/size\n",
        "acc = acc/size\n",
        "print(f'loss: {loss}\\nacc: {acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g34tZsxpUkrK"
      },
      "source": [
        "pbar = tqdm(validationloader, unit=\"audios\", unit_scale=validationloader.batch_size)\n",
        "size = validationloader.__len__()\n",
        "\n",
        "RNN_percentage = [] \n",
        "for batch in pbar:\n",
        "    inputs = batch[0]\n",
        "    targets = batch[1]\n",
        "    inputs = inputs.numpy()\n",
        "    targets = targets.numpy()\n",
        "    pred = list(model.predict(inputs))\n",
        "    RNN_percentage += pred\n",
        "\n",
        "RNN_percentage = np.asarray(RNN_percentage)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rYf7YRqVe3v"
      },
      "source": [
        "RNN_percentage.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G1g14-SBaFk"
      },
      "source": [
        "pbar = tqdm(testloader, unit=\"audios\", unit_scale=testloader.batch_size)\n",
        "size = validationloader.__len__()\n",
        "loss = 0   \n",
        "acc = 0\n",
        "for batch in pbar:\n",
        "    inputs = batch[0]\n",
        "    targets = batch[1]\n",
        "    inputs = inputs.numpy()\n",
        "    targets = targets.numpy()\n",
        "    metric = model.evaluate(inputs,targets,verbose=0)\n",
        "    loss += metric[0]\n",
        "    acc += metric[1]\n",
        "   \n",
        "loss = loss/size\n",
        "acc = acc/size\n",
        "print(f'loss: {loss}\\nacc: {acc}')\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvNJW4IL-Kmj"
      },
      "source": [
        "start_epoch = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfzlnrqjH0Wn"
      },
      "source": [
        "epochs=100   \n",
        "global_loss = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeAzO4fmzy2W"
      },
      "source": [
        "start_epoch   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWZ8k4DVz7SG"
      },
      "source": [
        "for epoch in range(start_epoch,epochs):\n",
        "  pbar = tqdm(trainloader, unit=\"audios\", unit_scale=trainloader.batch_size)\n",
        "  for batch in pbar:\n",
        "      inputs = batch[0]\n",
        "      targets = batch[1]\n",
        "      inputs = inputs.numpy()\n",
        "      targets = targets.numpy()\n",
        "      metric = model.train_on_batch(inputs,targets)\n",
        "\n",
        "      pbar.set_postfix({\n",
        "          'train loss': \"%f\" % (metric[0]),\n",
        "          'train acc': \"%f\" % (metric[1]),\n",
        "          'lr' : \"%.6f\" % (float(model.optimizer.learning_rate)),\n",
        "          'epoch': \"%d\" % (epoch)\n",
        "      })\n",
        "  K.set_value(model.optimizer.learning_rate, step_decay(epoch))  # set new learning_rate\n",
        "\n",
        "\n",
        "\n",
        "  pbar = tqdm(validationloader, unit=\"audios\", unit_scale=validationloader.batch_size)\n",
        "  size = validationloader.__len__()\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for batch in pbar:\n",
        "      inputs = batch[0]\n",
        "      targets = batch[1]\n",
        "      inputs = inputs.numpy()\n",
        "      targets = targets.numpy()\n",
        "      metric = model.evaluate(inputs,targets,verbose=0)\n",
        "      loss += metric[0]\n",
        "      acc += metric[1]\n",
        "\n",
        "  loss = loss/size\n",
        "  acc = acc/size\n",
        "  print(f'loss: {loss}\\nacc: {acc}')\n",
        "  pbar.set_postfix({\n",
        "    'loss': \"%f\" % (loss),\n",
        "    'acc': \"%f\" % (acc),\n",
        "  })\n",
        "  if loss < global_loss:\n",
        "    global_loss = loss\n",
        "    model.save(f'{MODEL_NAME}_bestloss.h5')\n",
        "    print('saving best loss')\n",
        "\n",
        "  model.save(f'{MODEL_NAME}_training.h5')\n",
        "                           \n",
        "                                      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJZ-je8PZGuy"
      },
      "source": [
        "for epoch in range(start_epoch,epochs):   \n",
        "  pbar = tqdm(semi_supervisedloader, unit=\"audios\", unit_scale=semi_supervisedloader.batch_size)\n",
        "  for batch in pbar:               \n",
        "      inputs = batch[0]       \n",
        "      targets = batch[1]      \n",
        "      inputs = inputs.numpy()   \n",
        "      targets = targets.numpy()\n",
        "      metric = model.train_on_batch(inputs,targets)\n",
        "\n",
        "      pbar.set_postfix({\n",
        "          'train loss': \"%f\" % (metric[0]),\n",
        "          'train acc': \"%f\" % (metric[1]),\n",
        "          'lr' : \"%.6f\" % (float(model.optimizer.learning_rate)),\n",
        "          'epoch': \"%d\" % (epoch)\n",
        "      })\n",
        "  K.set_value(model.optimizer.learning_rate, step_decay(epoch))  # set new learning_rate\n",
        "\n",
        "\n",
        "\n",
        "  pbar = tqdm(validationloader, unit=\"audios\", unit_scale=validationloader.batch_size)\n",
        "  size = validationloader.__len__()\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "  for batch in pbar:\n",
        "      inputs = batch[0]\n",
        "      targets = batch[1]\n",
        "      inputs = inputs.numpy()\n",
        "      targets = targets.numpy()\n",
        "      metric = model.evaluate(inputs,targets,verbose=0)\n",
        "      loss += metric[0]\n",
        "      acc += metric[1]\n",
        "\n",
        "  loss = loss/size\n",
        "  acc = acc/size\n",
        "  print(f'loss: {loss}\\nacc: {acc}')\n",
        "  pbar.set_postfix({\n",
        "    'loss': \"%f\" % (loss),\n",
        "    'acc': \"%f\" % (acc),\n",
        "  })\n",
        "  if loss < global_loss:\n",
        "    global_loss = loss\n",
        "    model.save(f'{MODEL_NAME}_bestloss.h5')\n",
        "    print('saving best loss')\n",
        "\n",
        "  model.save(f'{MODEL_NAME}_training.h5')\n",
        "                                                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVSJwxhL1ZSy"
      },
      "source": [
        "model.metrics_names                                                                                                             "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIBJBma9osef"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from kapre.time_frequency import Melspectrogram, Spectrogram\n",
        "from kapre.utils import Normalization2D\n",
        "from keras.models import Model, load_model\n",
        "\n",
        "from kapre.time_frequency import Melspectrogram, Spectrogram\n",
        "\n",
        "\n",
        "melspecModel = Sequential()\n",
        "iLen = 16000\n",
        "sr = 16000\n",
        "\n",
        "melspecModel.add(Melspectrogram(n_dft=1024, n_hop=128, input_shape=(1, iLen),\n",
        "                         padding='same', sr=sr, n_mels=80,\n",
        "                         fmin=40.0, fmax=sr/2, power_melgram=1.0,\n",
        "                         return_decibel_melgram=True, trainable_fb=False,\n",
        "                         trainable_kernel=False,\n",
        "                         name='mel_stft') )\n",
        "\n",
        "melspecModel.add(Normalization2D(int_axis=0))\n",
        "\n",
        "melspecModel.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVQay4H8prrO"
      },
      "source": [
        "melspec = melspecModel.predict( audio.reshape((-1,1,iLen)) )\n",
        "melspec.shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opOQz8MCp_vi"
      },
      "source": [
        "  \n",
        "plt.figure(figsize=(17,6))\n",
        "plt.pcolormesh(melspec[9,:,:,0])\n",
        "\n",
        "plt.title('Spectrogram visualization')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xlabel('Time')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4Tb0dIRNk6L"
      },
      "source": [
        "attSpeechModel = Model(inputs=model.input,\n",
        "                                 outputs=[model.get_layer('output').output, \n",
        "                                          model.get_layer('attSoftmax').output,\n",
        "                                          model.get_layer('mel_stft').output])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwvDk-vbQmUB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBLGhNh1NnZG"
      },
      "source": [
        "i = 0\n",
        "for audios_batch, classes_batch in validationloader:\n",
        "  audios_batch = audios_batch.numpy()\n",
        "  classes_batch = classes_batch.numpy()\n",
        "  if i == 0:\n",
        "    audios = audios_batch\n",
        "    classes = classes_batch\n",
        "  else:\n",
        "    audios = np.concatenate((audios,audios_batch))\n",
        "    classes = np.concatenate((classes,classes_batch))\n",
        "\n",
        "  i = i +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ztYYQpjRXof"
      },
      "source": [
        "audios.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvpzQpbVRaF7"
      },
      "source": [
        "model.evaluate(audios,classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW40En4fN_qh"
      },
      "source": [
        "idAudio = 1000\n",
        "classes[idAudio]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi3Vuyr3OvOH"
      },
      "source": [
        "outs, attW, specs = attSpeechModel.predict(audios)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5edUdBFiOzBS"
      },
      "source": [
        "np.argmax(outs,axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybkGhaPiO2ym"
      },
      "source": [
        "imgHeight = 2\n",
        "\n",
        "plt.figure(figsize=(17,imgHeight))\n",
        "plt.title('Raw waveform', fontsize=30)\n",
        "plt.ylabel('Amplitude', fontsize=30)\n",
        "plt.xlabel('Sample index', fontsize=30)\n",
        "plt.plot(audios[idAudio])\n",
        "plt.savefig('picrawWave.png', dpi = 400)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(17,imgHeight))\n",
        "plt.title('Attention weights (log)', fontsize=30)\n",
        "plt.ylabel('Log of attention weight', fontsize=30)\n",
        "plt.xlabel('Mel-spectrogram index', fontsize=30)\n",
        "plt.plot(np.log(attW[idAudio]))\n",
        "plt.savefig('picAttention.png', dpi = 400)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(17,imgHeight*2))\n",
        "plt.pcolormesh(specs[idAudio,:,:,0])\n",
        "\n",
        "plt.title('Spectrogram visualization', fontsize=30)\n",
        "plt.ylabel('Frequency', fontsize=30)\n",
        "plt.xlabel('Time', fontsize=30)\n",
        "plt.savefig('picmelSpec.png', dpi = 400)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RzTV4uLWCDu"
      },
      "source": [
        "predictions = (DPN92_percentage + RNN_percentage) / 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRMDKF4mQGqQ"
      },
      "source": [
        "predictions =  np.argmax(predictions,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9CM9K8ZP-s4"
      },
      "source": [
        "classes = truth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "waFk7LjpSCIJ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(classes,predictions, normalize='true')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA7BbZcWSNUA"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frsMrXSSTlSS"
      },
      "source": [
        "labels =  ['down',\n",
        "          'go',\n",
        "          'left',\n",
        "          'no',\n",
        "          'off',\n",
        "          'on',\n",
        "          'right',\n",
        "          'silence',\n",
        "          'stop',\n",
        "          'unknown',\n",
        "          'up',\n",
        "          'yes']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRlEjwBMTXiy"
      },
      "source": [
        "df_cm = pd.DataFrame(cm, index = [i for i in labels],\n",
        "                  columns = [i for i in labels])\n",
        "plt.figure(figsize = (20,20))\n",
        "sn.heatmap(df_cm, annot=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCT7yUjUUhiN"
      },
      "source": [
        "from sklearn.metrics import classification_report   \n",
        "print(classification_report(classes, predictions, target_names=labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFnickXyYttr"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(classes, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMr4fmPwYBqI"
      },
      "source": [
        "pbar = tqdm(submissionloader, unit=\"audios\", unit_scale=submissionloader.batch_size)\n",
        "i = 0\n",
        "files_name = []\n",
        "for batch in pbar:\n",
        "    inputs = batch[0]\n",
        "    names = batch[1]\n",
        "    inputs=inputs.numpy()\n",
        "    files_name += names\n",
        "    output = model.predict(inputs)\n",
        "    if i ==0:\n",
        "      predictions = np.argmax(output,1)\n",
        "    else:\n",
        "      predictions = np.concatenate((predictions,np.argmax(output,1)))\n",
        "    i+=1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8fMsGRgYzsf"
      },
      "source": [
        "predictions.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jWDAkBdahB6"
      },
      "source": [
        "# **Mix up**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-ZnSUmQajWE"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def mixup_cross_entropy_loss(input, target, size_average=True):\n",
        "    \"\"\"Origin: https://github.com/moskomule/mixup.pytorch\n",
        "    in PyTorch's cross entropy, targets are expected to be labels\n",
        "    so to predict probabilities this loss is needed\n",
        "    suppose q is the target and p is the input\n",
        "    loss(p, q) = -\\sum_i q_i \\log p_i\n",
        "    \"\"\"\n",
        "    assert input.size() == target.size()\n",
        "    assert isinstance(input, Variable) and isinstance(target, Variable)\n",
        "    input = torch.log(torch.nn.functional.softmax(input, dim=1).clamp(1e-5, 1))\n",
        "    # input = input - torch.log(torch.sum(torch.exp(input), dim=1)).view(-1, 1)\n",
        "    loss = - torch.sum(input * target)\n",
        "    return loss / input.size()[0] if size_average else loss\n",
        "\n",
        "def onehot(targets, num_classes):\n",
        "    \"\"\"Origin: https://github.com/moskomule/mixup.pytorch\n",
        "    convert index tensor into onehot tensor\n",
        "    :param targets: index tensor\n",
        "    :param num_classes: number of classes\n",
        "    \"\"\"\n",
        "    assert isinstance(targets, torch.LongTensor)\n",
        "    return torch.zeros(targets.size()[0], num_classes).scatter_(1, targets.view(-1, 1), 1)\n",
        "\n",
        "def mixup(inputs, targets, num_classes, alpha=2):\n",
        "    \"\"\"Mixup on 1x32x32 mel-spectrograms.\n",
        "    \"\"\"\n",
        "    s = inputs.size()[0]\n",
        "    weight = torch.Tensor(np.random.beta(alpha, alpha, s))\n",
        "    index = np.random.permutation(s)\n",
        "    x1, x2 = inputs, inputs[index, :, :, :]\n",
        "    y1, y2 = onehot(targets, num_classes), onehot(targets[index,], num_classes)\n",
        "    weight = weight.view(s, 1, 1, 1)\n",
        "    inputs = weight*x1 + (1-weight)*x2\n",
        "    weight = weight.view(s, 1)\n",
        "    targets = weight*y1 + (1-weight)*y2\n",
        "    return inputs, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-xZPoM6rItg"
      },
      "source": [
        "# **Resnext**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR9Zr70ztNBk"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Dataset/transfer_learning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0tUUAFAvHJN"
      },
      "source": [
        "MODEL_NAME = 'Resnextscipymixup'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrEtj0USTVx_"
      },
      "source": [
        "MIXUP = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOTYXlGkrMg2"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfvTH7hbrQQF"
      },
      "source": [
        "class ResNeXtBottleneck(nn.Module):\n",
        "    \"\"\"\n",
        "    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride, cardinality, base_width, widen_factor):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            in_channels: input channel dimensionality\n",
        "            out_channels: output channel dimensionality\n",
        "            stride: conv stride. Replaces pooling layer.\n",
        "            cardinality: num of convolution groups.\n",
        "            base_width: base number of channels in each group.\n",
        "            widen_factor: factor to reduce the input dimensionality before convolution.\n",
        "        \"\"\"\n",
        "        super(ResNeXtBottleneck, self).__init__()\n",
        "        width_ratio = out_channels / (widen_factor * 64.)\n",
        "        D = cardinality * int(base_width * width_ratio)\n",
        "        self.conv_reduce = nn.Conv2d(in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn_reduce = nn.BatchNorm2d(D)\n",
        "        self.conv_conv = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(D)\n",
        "        self.conv_expand = nn.Conv2d(D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn_expand = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut.add_module('shortcut_conv',\n",
        "                                     nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0,\n",
        "                                               bias=False))\n",
        "            self.shortcut.add_module('shortcut_bn', nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        bottleneck = self.conv_reduce.forward(x)\n",
        "        bottleneck = F.relu(self.bn_reduce.forward(bottleneck), inplace=True)\n",
        "        bottleneck = self.conv_conv.forward(bottleneck)\n",
        "        bottleneck = F.relu(self.bn.forward(bottleneck), inplace=True)\n",
        "        bottleneck = self.conv_expand.forward(bottleneck)\n",
        "        bottleneck = self.bn_expand.forward(bottleneck)\n",
        "        residual = self.shortcut.forward(x)\n",
        "        return F.relu(residual + bottleneck, inplace=True)\n",
        "\n",
        "\n",
        "class CifarResNeXt(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNext optimized for the Cifar dataset, as specified in\n",
        "    https://arxiv.org/pdf/1611.05431.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nlabels, cardinality=8, depth=29, base_width=64, widen_factor=4, in_channels=3):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            cardinality: number of convolution groups.\n",
        "            depth: number of layers.\n",
        "            nlabels: number of classes\n",
        "            base_width: base number of channels in each group.\n",
        "            widen_factor: factor to adjust the channel dimensionality\n",
        "        \"\"\"\n",
        "        super(CifarResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.depth = depth\n",
        "        self.block_depth = (self.depth - 2) // 9\n",
        "        self.base_width = base_width\n",
        "        self.widen_factor = widen_factor\n",
        "        self.nlabels = nlabels\n",
        "        self.output_size = 64\n",
        "        self.stages = [64, 64 * self.widen_factor, 128 * self.widen_factor, 256 * self.widen_factor]\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(in_channels, 64, 3, 1, 1, bias=False)\n",
        "        self.bn_1 = nn.BatchNorm2d(64)\n",
        "        self.stage_1 = self.block('stage_1', self.stages[0], self.stages[1], 1)\n",
        "        self.stage_2 = self.block('stage_2', self.stages[1], self.stages[2], 2)\n",
        "        self.stage_3 = self.block('stage_3', self.stages[2], self.stages[3], 2)\n",
        "        self.classifier = nn.Linear(self.stages[3], nlabels)\n",
        "        init.kaiming_normal_(self.classifier.weight)\n",
        "\n",
        "        for key in self.state_dict():\n",
        "            if key.split('.')[-1] == 'weight':\n",
        "                if 'conv' in key:\n",
        "                    init.kaiming_normal_(self.state_dict()[key], mode='fan_out')\n",
        "                if 'bn' in key:\n",
        "                    self.state_dict()[key][...] = 1\n",
        "            elif key.split('.')[-1] == 'bias':\n",
        "                self.state_dict()[key][...] = 0\n",
        "\n",
        "    def block(self, name, in_channels, out_channels, pool_stride=2):\n",
        "        \"\"\" Stack n bottleneck modules where n is inferred from the depth of the network.\n",
        "        Args:\n",
        "            name: string name of the current block.\n",
        "            in_channels: number of input channels\n",
        "            out_channels: number of output channels\n",
        "            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n",
        "        Returns: a Module consisting of n sequential bottlenecks.\n",
        "        \"\"\"\n",
        "        block = nn.Sequential()\n",
        "        for bottleneck in range(self.block_depth):\n",
        "            name_ = '%s_bottleneck_%d' % (name, bottleneck)\n",
        "            if bottleneck == 0:\n",
        "                block.add_module(name_, ResNeXtBottleneck(in_channels, out_channels, pool_stride, self.cardinality,\n",
        "                                                          self.base_width, self.widen_factor))\n",
        "            else:\n",
        "                block.add_module(name_,\n",
        "                                 ResNeXtBottleneck(out_channels, out_channels, 1, self.cardinality, self.base_width,\n",
        "                                                   self.widen_factor))\n",
        "        return block\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1_3x3.forward(x)\n",
        "        x = F.relu(self.bn_1.forward(x), inplace=True)\n",
        "        x = self.stage_1.forward(x)\n",
        "        x = self.stage_2.forward(x)\n",
        "        x = self.stage_3.forward(x)\n",
        "        x = F.avg_pool2d(x, 8, 1)\n",
        "        x = x.view(-1, self.stages[3])\n",
        "        return self.classifier(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0060rU1r61A"
      },
      "source": [
        "model = CifarResNeXt(nlabels=12, in_channels=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zHNuj17Xbhr"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vu3tVi0sLXA"
      },
      "source": [
        "if use_gpu:\n",
        "    model = torch.nn.DataParallel(model).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbR9qWhqsL8-"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMasamprsnJ2"
      },
      "source": [
        "start_epoch = 0\n",
        "best_accuracy = 0\n",
        "best_loss = 1e100\n",
        "global_step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2nyPAuOtJsZ"
      },
      "source": [
        "resume = 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNQvQKAcu-OG"
      },
      "source": [
        "if resume:\n",
        "    checkpoint = torch.load(f'{MODEL_NAME}_training.pth')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.float()\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    best_accuracy = checkpoint['accuracy']\n",
        "    best_loss = checkpoint['loss']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    global_step = checkpoint['step']\n",
        "\n",
        "    del checkpoint  # reduce memory\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihpclMp171lG"
      },
      "source": [
        "best_accuracy,best_loss,start_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf5vpJiftwXq"
      },
      "source": [
        "best_accuracy,best_loss,start_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJMuChV3t1t6"
      },
      "source": [
        "best_accuracy,best_loss,start_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwkgeXj-EWI4"
      },
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvBJOBk_EYt_"
      },
      "source": [
        "pytorch_total_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NIdrqQZvUBx"
      },
      "source": [
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B78LCCpyvxJg"
      },
      "source": [
        "def get_lr():\n",
        "    return optimizer.param_groups[0]['lr']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSp6eecYTy1E"
      },
      "source": [
        "def train(epoch,mode):\n",
        "    global global_step\n",
        "\n",
        "    print(\"epoch %3d with lr=%.02e\" % (epoch, get_lr()))\n",
        "    phase = 'train'\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0       \n",
        "    it = 0\n",
        "    correct = 0    \n",
        "    total = 0\n",
        "    if mode == 'train':\n",
        "      pbar = tqdm(trainloader, unit=\"audios\", unit_scale=trainloader.batch_size)\n",
        "    elif mode == 'semi_supervised':\n",
        "      pbar = tqdm(semi_supervisedloader, unit=\"audios\", unit_scale=semi_supervisedloader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "        inputs=inputs.float()\n",
        "\n",
        "        if MIXUP:\n",
        "          inputs, targets = mixup(inputs, targets, num_classes=12)\n",
        "\n",
        "        inputs = Variable(inputs, requires_grad=True)\n",
        "        targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda(async=True)\n",
        "\n",
        "        # forward/backward\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        if MIXUP:\n",
        "            loss = mixup_cross_entropy_loss(outputs, targets)\n",
        "        else:\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        it += 1\n",
        "        global_step += 1\n",
        "        running_loss += loss.item()\n",
        "        pred = outputs.data.max(1, keepdim=True)[1]\n",
        "\n",
        "\n",
        "\n",
        "        if MIXUP:\n",
        "            targets = batch[1]\n",
        "            targets = Variable(targets, requires_grad=False).cuda(async=True)\n",
        "\n",
        "\n",
        "\n",
        "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "        total += targets.size(0)\n",
        "\n",
        "\n",
        "        # update the progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': \"%.05f\" % (running_loss / it),\n",
        "            'acc': \"%.02f%%\" % (100*correct/total)\n",
        "        })\n",
        "\n",
        "    accuracy = int(correct)/total\n",
        "    epoch_loss = running_loss / it\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAMH4M5Iv7mv"
      },
      "source": [
        "def valid(epoch):\n",
        "    global best_accuracy, best_loss, global_step\n",
        "\n",
        "    phase = 'valid'\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    it = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(validationloader, unit=\"audios\", unit_scale=validationloader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "        inputs=inputs.float()\n",
        "        #inputs = Variable(inputs, volatile = True)\n",
        "        #targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if use_gpu:\n",
        "              inputs = inputs.cuda()\n",
        "              targets = targets.cuda(async=True)\n",
        "\n",
        "          # forward\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          # statistics\n",
        "          it += 1\n",
        "          global_step += 1\n",
        "          running_loss += loss.item()\n",
        "          pred = outputs.data.max(1, keepdim=True)[1]\n",
        "          correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "          total += targets.size(0)\n",
        "\n",
        "\n",
        "          # update the progress bar\n",
        "          pbar.set_postfix({\n",
        "              'loss': \"%.05f\" % (running_loss / it),\n",
        "              'acc': \"%.02f%%\" % (100*correct/total)\n",
        "          })\n",
        "\n",
        "    accuracy = int(correct)/total\n",
        "    epoch_loss = running_loss / it\n",
        "\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'step': global_step,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(checkpoint, f'{MODEL_NAME}_bestacc.pth')\n",
        "        torch.save(model, f'{MODEL_NAME}_bestacc_modelonly.pth')\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(checkpoint, f'{MODEL_NAME}_bestloss.pth')\n",
        "        torch.save(model, f'{MODEL_NAME}_bestloss_modelonly.pth')\n",
        "\n",
        "    torch.save(checkpoint, f'{MODEL_NAME}_training.pth')\n",
        "    del checkpoint  # reduce memory\n",
        "\n",
        "    return epoch_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I826cgG-td0G"
      },
      "source": [
        "def test():\n",
        "\n",
        "    phase = 'test'\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    it = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(testloader, unit=\"audios\", unit_scale=testloader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "        inputs=inputs.float()\n",
        "        #inputs = Variable(inputs, volatile = True)\n",
        "        #targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if use_gpu:\n",
        "              inputs = inputs.cuda()\n",
        "              targets = targets.cuda(async=True)\n",
        "\n",
        "          # forward\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          # statistics\n",
        "          it += 1\n",
        "          running_loss += loss.item()\n",
        "          pred = outputs.data.max(1, keepdim=True)[1]\n",
        "          correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "          total += targets.size(0)\n",
        "\n",
        "\n",
        "          # update the progress bar\n",
        "          pbar.set_postfix({\n",
        "              'loss': \"%.05f\" % (running_loss / it),\n",
        "              'acc': \"%.02f%%\" % (100*correct/total)\n",
        "          })\n",
        "\n",
        "    accuracy = int(correct)/total\n",
        "    epoch_loss = running_loss / it\n",
        "\n",
        "    print(f\"acc is {accuracy}\")\n",
        "    print(f\"loss is {epoch_loss}\")\n",
        "\n",
        "\n",
        "    return epoch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y4BT7Daclzn"
      },
      "source": [
        "def submission():\n",
        "    phase = 'submission'\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "\n",
        "\n",
        "\n",
        "    files_name = []\n",
        "    predictions = []\n",
        "\n",
        "    pbar = tqdm(submissionloader, unit=\"audios\", unit_scale=submissionloader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        names = batch[1]\n",
        "        inputs=inputs.float()\n",
        "        #inputs = Variable(inputs, volatile = True)\n",
        "        #targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "        files_name += names\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if use_gpu:\n",
        "              inputs = inputs.cuda()\n",
        "\n",
        "          # forward\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # statistics\n",
        "          pred = outputs.data.max(1, keepdim=True)[1]\n",
        "          temp_pred = []\n",
        "          for i in range(pred.shape[0]):\n",
        "            temp_pred.append(int(pred[i]))\n",
        "        \n",
        "          predictions += temp_pred\n",
        "\n",
        "          #break\n",
        "\n",
        "    return files_name,predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEBVSE3cnEye"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m2U805l_A7Q"
      },
      "source": [
        "files_name,predictions = submission()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbFAypr8aQV0"
      },
      "source": [
        "len(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-A0ApcbBWOW"
      },
      "source": [
        "classes=  {'down':0,\n",
        "          'go':1,\n",
        "          'left':2,\n",
        "          'no':3,\n",
        "          'off':4,\n",
        "          'on':5,\n",
        "          'right':6,   \n",
        "          'silence':7,    \n",
        "          'stop':8,\n",
        "          'unknown':9,\n",
        "          'up':10,   \n",
        "          'yes':11,}       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-32WTQcYInA4"
      },
      "source": [
        "predictions_str = []\n",
        "for i in range(len(predictions)):\n",
        "  pred = predictions[i]\n",
        "  listOfKeys = [key  for (key, value) in classes.items() if value == pred]\n",
        "  predictions_str.append(listOfKeys[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEE930gXBvQy"
      },
      "source": [
        "def write_submission(files_name, predictions, submission_file):\n",
        "    with open(f\"/content/{submission_file}.txt\",'w') as fid:\n",
        "        fid.write(\"fname,label\\n\")\n",
        "        for i in range(len(files_name)):\n",
        "            wav_name = files_name[i]\n",
        "            wav_label = predictions[i]\n",
        "            fid.write(f\"{wav_name},{wav_label}\\n\")   \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_btEm9VJIpu"
      },
      "source": [
        "write_submission(files_name, predictions_str, 'submission_lstm_32_32')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2tgfu7OlGda"
      },
      "source": [
        "for epoch in range(start_epoch, 140):\n",
        "\n",
        "    train(epoch,'train')     \n",
        "    epoch_loss = valid(epoch)\n",
        "    lr_scheduler.step(metrics=epoch_loss)  \n",
        "           \n",
        "                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iwBogKNEEyt"
      },
      "source": [
        "# **DPN92**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDVDpiwZEC4i"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Dataset/transfer_learning   \n",
        "!ls                                     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU_tTiGeEC4o"
      },
      "source": [
        "MODEL_NAME = 'DPN92GAN'              "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ydZ164RcNmh"
      },
      "source": [
        "MIXUP = 0        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCdZwEpDEC4s"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAYMnNi1EC4v"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.out_planes = out_planes\n",
        "        self.dense_depth = dense_depth\n",
        "\n",
        "        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if first_layer:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_planes+dense_depth)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        x = self.shortcut(x)\n",
        "        d = self.out_planes\n",
        "        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DPN(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels, cfg):\n",
        "        super(DPN, self).__init__()\n",
        "        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n",
        "        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.last_planes = 64\n",
        "        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n",
        "        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n",
        "        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n",
        "        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n",
        "        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for i,stride in enumerate(strides):\n",
        "            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n",
        "            self.last_planes = out_planes + (i+2) * dense_depth\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def DPN92(num_classes, in_channels=3):\n",
        "    cfg = {\n",
        "        'in_planes': (96,192,384,768),\n",
        "        'out_planes': (256,512,1024,2048),\n",
        "        'num_blocks': (3,4,20,3),\n",
        "        'dense_depth': (16,32,24,128)\n",
        "    }\n",
        "    return DPN(num_classes, in_channels, cfg)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llwUIYCXEC4y"
      },
      "source": [
        "model = DPN92(num_classes=12, in_channels=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3z5sxmgcA7_"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Meklb-SvEC42"
      },
      "source": [
        "if use_gpu:\n",
        "    model = torch.nn.DataParallel(model).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQInIxBWEC46"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cA6kKvzzEC4_"
      },
      "source": [
        "start_epoch = 0\n",
        "best_accuracy = 0\n",
        "best_loss = 1e100\n",
        "global_step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW-McO4MEC5D"
      },
      "source": [
        "resume = 1 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXjXbP7iEC5G"
      },
      "source": [
        "if resume:\n",
        "    checkpoint = torch.load(f'{MODEL_NAME}_bestloss.pth')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.float()\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    best_accuracy = checkpoint['accuracy']\n",
        "    best_loss = checkpoint['loss']\n",
        "    start_epoch = checkpoint['epoch']\n",
        "    global_step = checkpoint['step']\n",
        "\n",
        "    del checkpoint  # reduce memory\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81RwSk8NEC5I"
      },
      "source": [
        "best_accuracy,best_loss,start_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmGvryfwEC5N"
      },
      "source": [
        "best_accuracy,best_loss,start_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U39VgoeEC5U"
      },
      "source": [
        "best_accuracy,best_loss,start_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRfJBgBqEC5Y"
      },
      "source": [
        "pytorch_total_params = sum(p.numel() for p in model.parameters())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cEO0Y9GEC5c"
      },
      "source": [
        "pytorch_total_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZZCUlbvEC5e"
      },
      "source": [
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib6noX81EC5h"
      },
      "source": [
        "def get_lr():\n",
        "    return optimizer.param_groups[0]['lr']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn6xT4V-EC5k"
      },
      "source": [
        "def train(epoch,mode):\n",
        "    global global_step\n",
        "\n",
        "    print(\"epoch %3d with lr=%.02e\" % (epoch, get_lr()))\n",
        "    phase = 'train'\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    it = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    if mode == 'train':\n",
        "      pbar = tqdm(trainloader, unit=\"audios\", unit_scale=trainloader.batch_size)\n",
        "    elif mode == 'semi_supervised':\n",
        "      pbar = tqdm(semi_supervisedloader, unit=\"audios\", unit_scale=semi_supervisedloader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "        inputs=inputs.float()\n",
        "\n",
        "        if MIXUP:\n",
        "          inputs, targets = mixup(inputs, targets, num_classes=12)\n",
        "\n",
        "        inputs = Variable(inputs, requires_grad=True)\n",
        "        targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda(async=True)\n",
        "\n",
        "        # forward/backward\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        if MIXUP:\n",
        "            loss = mixup_cross_entropy_loss(outputs, targets)\n",
        "        else:\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        it += 1\n",
        "        global_step += 1\n",
        "        running_loss += loss.item()\n",
        "        pred = outputs.data.max(1, keepdim=True)[1]\n",
        "\n",
        "\n",
        "\n",
        "        if MIXUP:\n",
        "            targets = batch[1]\n",
        "            targets = Variable(targets, requires_grad=False).cuda(async=True)\n",
        "\n",
        "\n",
        "\n",
        "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "        total += targets.size(0)\n",
        "\n",
        "\n",
        "        # update the progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': \"%.05f\" % (running_loss / it),\n",
        "            'acc': \"%.02f%%\" % (100*correct//total)\n",
        "        })\n",
        "\n",
        "    accuracy = int(correct)/total\n",
        "    epoch_loss = running_loss / it\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be_occ3mEC5n"
      },
      "source": [
        "def valid(epoch):\n",
        "    global best_accuracy, best_loss, global_step\n",
        "\n",
        "    phase = 'valid'\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    it = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(validationloader, unit=\"audios\", unit_scale=validationloader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "        inputs=inputs.float()\n",
        "        #inputs = Variable(inputs, volatile = True)\n",
        "        #targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if use_gpu:\n",
        "              inputs = inputs.cuda()\n",
        "              targets = targets.cuda(async=True)\n",
        "\n",
        "          # forward\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          # statistics\n",
        "          it += 1\n",
        "          global_step += 1\n",
        "          running_loss += loss.item()\n",
        "          pred = outputs.data.max(1, keepdim=True)[1]\n",
        "          correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "          total += targets.size(0)\n",
        "\n",
        "\n",
        "          # update the progress bar\n",
        "          pbar.set_postfix({\n",
        "              'loss': \"%.05f\" % (running_loss / it),\n",
        "              'acc': \"%.02f%%\" % (100*correct//total)\n",
        "          })\n",
        "\n",
        "    accuracy = int(correct)/total\n",
        "    epoch_loss = running_loss / it\n",
        "\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'step': global_step,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(checkpoint, f'{MODEL_NAME}_bestacc.pth')\n",
        "        torch.save(model, f'{MODEL_NAME}_bestacc_modelonly.pth')\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(checkpoint, f'{MODEL_NAME}_bestloss.pth')\n",
        "        torch.save(model, f'{MODEL_NAME}_bestloss_modelonly.pth')\n",
        "\n",
        "    torch.save(checkpoint, f'{MODEL_NAME}_training.pth')\n",
        "    del checkpoint  # reduce memory\n",
        "\n",
        "    return epoch_loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnSf60DiEC5p"
      },
      "source": [
        "def test():\n",
        "\n",
        "    phase = 'test'\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    it = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(testloader, unit=\"audios\", unit_scale=testloader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "        inputs=inputs.float()\n",
        "        #inputs = Variable(inputs, volatile = True)\n",
        "        #targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if use_gpu:\n",
        "              inputs = inputs.cuda()\n",
        "              targets = targets.cuda(async=True)\n",
        "\n",
        "          # forward\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "\n",
        "          # statistics\n",
        "          it += 1\n",
        "          running_loss += loss.item()\n",
        "          pred = outputs.data.max(1, keepdim=True)[1]\n",
        "          correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "          total += targets.size(0)\n",
        "\n",
        "\n",
        "          # update the progress bar\n",
        "          pbar.set_postfix({\n",
        "              'loss': \"%.05f\" % (running_loss / it),\n",
        "              'acc': \"%.02f%%\" % (100*correct/total)\n",
        "          })\n",
        "\n",
        "    accuracy = int(correct)/total\n",
        "    epoch_loss = running_loss / it\n",
        "\n",
        "    print(f\"acc is {accuracy}\")\n",
        "    print(f\"loss is {epoch_loss}\")\n",
        "\n",
        "\n",
        "    return epoch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq6j-0moNS8n"
      },
      "source": [
        "def evaluate(loader):\n",
        "\n",
        "    phase = 'test'\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "    predictions = []\n",
        "    truth = []\n",
        "\n",
        "    pbar = tqdm(loader, unit=\"audios\", unit_scale=loader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "        inputs=inputs.float()\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if use_gpu:\n",
        "              inputs = inputs.cuda()\n",
        "              targets = targets.cuda(async=True)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          pred = outputs.data.max(1, keepdim=True)[1]\n",
        "\n",
        "          temp_pred = []\n",
        "          for i in range(pred.shape[0]):\n",
        "            temp_pred.append(int(pred[i]))\n",
        "        \n",
        "          predictions += temp_pred\n",
        "          \n",
        "          temp_targets = []\n",
        "          for i in range(targets.shape[0]):\n",
        "            temp_targets.append(int(targets[i]))\n",
        "          truth += temp_targets\n",
        "\n",
        "\n",
        "    return predictions,truth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqJy38oFRhfH"
      },
      "source": [
        "def get_prediction_vector(loader):\n",
        "\n",
        "    phase = 'test'\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "    predictions = []\n",
        "\n",
        "    pbar = tqdm(loader, unit=\"audios\", unit_scale=loader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "        inputs=inputs.float()\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if use_gpu:\n",
        "              inputs = inputs.cuda()\n",
        "              targets = targets.cuda(async=True)\n",
        "\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          pred = list(outputs.data.cpu().detach().numpy())#.max(1, keepdim=True)[1]\n",
        "          \"\"\"\n",
        "          pred_temp = []\n",
        "          for i in range(pred.shape[0]):\n",
        "            pred_temp.append(pred[i].cpu().detach().numpy())\n",
        "          \"\"\"\n",
        "          predictions += pred\n",
        "\n",
        "\n",
        "\n",
        "    return np.asarray(predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwQdJKwsOY_L"
      },
      "source": [
        "predictions,truth = evaluate(validationloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqKEbfqvR94l"
      },
      "source": [
        "DPN92_percentage = get_prediction_vector(validationloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsots3I6SH35"
      },
      "source": [
        "DPN92_percentage[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqQKBxFfO8RY"
      },
      "source": [
        "predictions[0],truth[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMhsywQSEC5r"
      },
      "source": [
        "def submission():\n",
        "    phase = 'submission'\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "\n",
        "\n",
        "\n",
        "    files_name = []\n",
        "    predictions = []\n",
        "\n",
        "    pbar = tqdm(submissionloader, unit=\"audios\", unit_scale=submissionloader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        names = batch[1]\n",
        "        inputs=inputs.float()\n",
        "        #inputs = Variable(inputs, volatile = True)\n",
        "        #targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "        files_name += names\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "          if use_gpu:\n",
        "              inputs = inputs.cuda()\n",
        "\n",
        "          # forward\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # statistics\n",
        "          pred = outputs.data.max(1, keepdim=True)[1]\n",
        "          temp_pred = []\n",
        "          for i in range(pred.shape[0]):\n",
        "            temp_pred.append(int(pred[i]))\n",
        "        \n",
        "          predictions += temp_pred\n",
        "\n",
        "          #break\n",
        "\n",
        "    return files_name,predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX7pPpi1EC5t"
      },
      "source": [
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9chC_sdEC5v"
      },
      "source": [
        "files_name,predictions = submission()   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6Q9qzl3EC5x"
      },
      "source": [
        "classes=  {'down':0,\n",
        "          'go':1,\n",
        "          'left':2,\n",
        "          'no':3,\n",
        "          'off':4,\n",
        "          'on':5,\n",
        "          'right':6,   \n",
        "          'silence':7,    \n",
        "          'stop':8,\n",
        "          'unknown':9,\n",
        "          'up':10,   \n",
        "          'yes':11,}       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUpQpjDrEC52"
      },
      "source": [
        "predictions_str = []\n",
        "for i in range(len(predictions)):\n",
        "  pred = predictions[i]\n",
        "  listOfKeys = [key  for (key, value) in classes.items() if value == pred]\n",
        "  predictions_str.append(listOfKeys[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzauXL2hEC55"
      },
      "source": [
        "def write_submission(files_name, predictions, submission_file):\n",
        "    with open(f\"/content/{submission_file}.txt\",'w') as fid:\n",
        "        fid.write(\"fname,label\\n\")\n",
        "        for i in range(len(files_name)):\n",
        "            wav_name = files_name[i]\n",
        "            wav_label = predictions[i]\n",
        "            fid.write(f\"{wav_name},{wav_label}\\n\")   \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAtutP44EC58"
      },
      "source": [
        "write_submission(files_name, predictions_str, 'submission_resnext_semi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tcDjVnjDGUA"
      },
      "source": [
        "MODEL_NAME"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pz3vtrMyh45I"
      },
      "source": [
        "for epoch in range(start_epoch, 140):\n",
        "\n",
        "    train(epoch,'train')       \n",
        "    epoch_loss = valid(epoch)\n",
        "    lr_scheduler.step(metrics=epoch_loss)  \n",
        "                                               \n",
        "                                        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TVWuYP0ZSgF"
      },
      "source": [
        "# densenet_bc_100_12"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g76zrwBZVXU"
      },
      "source": [
        "import torch.nn as nn        \n",
        "import torch.nn.functional as F                                        \n",
        "from torch.nn import init                                                                                                                         \n",
        "import time                                                                                                                                 \n",
        "import math                                                                                                                                           \n",
        "from tqdm.notebook import tqdm\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1rY0UKvZdRY"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Nwishy/transfer_learning/densenet_bc_100_12_scipy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UFhk79kZf4b"
      },
      "source": [
        "__all__ = [ 'DenseNet' ]\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, inplanes, expansion=4, growthRate=12, dropRate=0):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        planes = expansion * growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropRate = dropRate\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        if self.dropRate > 0:\n",
        "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
        "\n",
        "        out = torch.cat((x, out), 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, expansion=1, growthRate=12, dropRate=0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        planes = expansion * growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropRate = dropRate\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        if self.dropRate > 0:\n",
        "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
        "\n",
        "        out = torch.cat((x, out), 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, inplanes, outplanes):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "\n",
        "    def __init__(self, depth=22, block=Bottleneck,\n",
        "        dropRate=0, num_classes=10, growthRate=12, compressionRate=2, in_channels=3):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        assert (depth - 4) % 3 == 0, 'depth should be 3n+4'\n",
        "        n = (depth - 4) / 3 if block == BasicBlock else (depth - 4) // 6\n",
        "\n",
        "        self.growthRate = growthRate\n",
        "        self.dropRate = dropRate\n",
        "\n",
        "        # self.inplanes is a global variable used across multiple\n",
        "        # helper functions\n",
        "        self.inplanes = growthRate * 2\n",
        "        self.conv1 = nn.Conv2d(in_channels, self.inplanes, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.dense1 = self._make_denseblock(block, n)\n",
        "        self.trans1 = self._make_transition(compressionRate)\n",
        "        self.dense2 = self._make_denseblock(block, n)\n",
        "        self.trans2 = self._make_transition(compressionRate)\n",
        "        self.dense3 = self._make_denseblock(block, n)\n",
        "        self.bn = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(self.inplanes, num_classes)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_denseblock(self, block, blocks):\n",
        "        layers = []\n",
        "        for i in range(blocks):\n",
        "            # Currently we fix the expansion ratio as the default value\n",
        "            layers.append(block(self.inplanes, growthRate=self.growthRate, dropRate=self.dropRate))\n",
        "            self.inplanes += self.growthRate\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_transition(self, compressionRate):\n",
        "        inplanes = self.inplanes\n",
        "        outplanes = int(math.floor(self.inplanes // compressionRate))\n",
        "        self.inplanes = outplanes\n",
        "        return Transition(inplanes, outplanes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.trans1(self.dense1(x))\n",
        "        x = self.trans2(self.dense2(x))\n",
        "        x = self.dense3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQR1KjU_Zom1"
      },
      "source": [
        "num_classes = 12\n",
        "in_channels = 1\n",
        "model = DenseNet(depth=100, growthRate=12, compressionRate=2, num_classes=num_classes, in_channels=in_channels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IyLf0RYZsDk"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiqLX6d3Zt0X"
      },
      "source": [
        "if use_gpu:\n",
        "    model = torch.nn.DataParallel(model).cuda()\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXCVuo_TZxlB"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6osawuMZzST"
      },
      "source": [
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFKRPfZQZ08_"
      },
      "source": [
        "def get_lr():\n",
        "    return optimizer.param_groups[0]['lr']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmMZmk21Z3BB"
      },
      "source": [
        "start_timestamp = int(time.time()*1000)\n",
        "start_epoch = 0\n",
        "best_accuracy = 0\n",
        "best_loss = 1e100\n",
        "global_step = 0\n",
        "#learning_rate = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnX7atKTZ-Vt"
      },
      "source": [
        "Resume = 1 if I want to load a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACkwhfWjZ5u7"
      },
      "source": [
        "resume = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOaC8wxQaAPQ"
      },
      "source": [
        "if resume:\n",
        "    print(\"resuming from a checkpoint\")\n",
        "    checkpoint = torch.load('densenet_bc_100_12_resume_12_classes.pth')\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    model.float()\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "\n",
        "    best_accuracy = checkpoint.get('accuracy', best_accuracy)\n",
        "    best_loss = checkpoint.get('loss', best_loss)\n",
        "    start_epoch = checkpoint.get('epoch', start_epoch)\n",
        "    global_step = checkpoint.get('step', global_step)\n",
        "\n",
        "    print(f'best_accuracy is : {best_accuracy}')\n",
        "    print(f'best loss is : {best_loss}')\n",
        "    print(f'I am in the epoch: {start_epoch}')\n",
        "    print(f'I have traveled : {global_step} steps')\n",
        "\n",
        "    del checkpoint  # reduce memory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njRA_9Z5aDib"
      },
      "source": [
        "def train(epoch):\n",
        "    global global_step\n",
        "\n",
        "    print(\"Training in epoch %3d with lr=%.02e\" % (epoch, get_lr()))\n",
        "    #learning_rate.append(get_lr())\n",
        "\n",
        "    model.train()  # Set model to training mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    it = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(trainloader, unit=\"audios\", unit_scale=trainloader.batch_size)\n",
        "    \n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "\n",
        "        # Look for the mixup method please\n",
        "        #if args.mixup:\n",
        "            #inputs, targets = mixup(inputs, targets, num_classes=len(CLASSES))\n",
        "\n",
        "        inputs = Variable(inputs, requires_grad=True)\n",
        "        targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda(async=True)\n",
        "\n",
        "        # forward/backward\n",
        "        outputs = model(inputs)\n",
        "        #if args.mixup:\n",
        "            #loss = mixup_cross_entropy_loss(outputs, targets)\n",
        "        #else:\n",
        "        loss = criterion(outputs, targets)\n",
        "        #####################################\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # statistics\n",
        "        it += 1\n",
        "        global_step += 1\n",
        "        running_loss += loss.item()\n",
        "        pred = outputs.data.max(1, keepdim=True)[1]\n",
        "        #if args.mixup:\n",
        "            #targets = batch['target']\n",
        "            #targets = Variable(targets, requires_grad=False).cuda(async=True)\n",
        "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "        total += targets.size(0)\n",
        "\n",
        "        #writer.add_scalar('%s/loss' % phase, loss.data[0], global_step)\n",
        "\n",
        "        # update the progress bar\n",
        "        pbar.set_postfix({\n",
        "            'train loss': \"%.05f\" % (running_loss / it),\n",
        "            'acc': \"%.02f%%\" % (100*correct/total)\n",
        "        })\n",
        "\n",
        "    accuracy = correct/total\n",
        "    epoch_loss = running_loss / it\n",
        "    #writer.add_scalar('%s/accuracy' % phase, 100*accuracy, epoch)\n",
        "    #writer.add_scalar('%s/epoch_loss' % phase, epoch_loss, epoch)\n",
        "    #print('Training loss: %.05f' % (running_loss / it))\n",
        "    #print('Training acc \"%.02f%%' % (100*correct/total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSrAgVGuaH80"
      },
      "source": [
        "def valid(epoch):\n",
        "    global best_accuracy, best_loss, global_step\n",
        "\n",
        "    #phase = 'valid'\n",
        "    model.eval()  # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    it = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(validationloader, unit=\"audios\", unit_scale=validationloader.batch_size)\n",
        "    for batch in pbar:\n",
        "        inputs = batch[0]\n",
        "        inputs = torch.unsqueeze(inputs, 1)\n",
        "        targets = batch[1]\n",
        "\n",
        "        inputs = Variable(inputs, volatile = True)\n",
        "        targets = Variable(targets, requires_grad=False)\n",
        "\n",
        "        if use_gpu:\n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda(async=True)\n",
        "\n",
        "        # forward\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # statistics\n",
        "        it += 1\n",
        "        global_step += 1\n",
        "        running_loss += loss.item()\n",
        "        pred = outputs.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(targets.data.view_as(pred)).sum()\n",
        "        total += targets.size(0)\n",
        "\n",
        "        #writer.add_scalar('%s/loss' % phase, loss.data[0], global_step)\n",
        "\n",
        "        # update the progress bar\n",
        "        pbar.set_postfix({\n",
        "            'validatio loss': \"%.05f\" % (running_loss / it),\n",
        "            'acc': \"%.02f%%\" % (100*correct/total)\n",
        "        })\n",
        "\n",
        "    accuracy = correct/total\n",
        "    epoch_loss = running_loss / it\n",
        "    #writer.add_scalar('%s/accuracy' % phase, 100*accuracy, epoch)\n",
        "    #writer.add_scalar('%s/epoch_loss' % phase, epoch_loss, epoch)\n",
        "    #print('Validation acc is: {accuracy}')\n",
        "    #print('validation loss is: {epoch_loss}')\n",
        "\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'step': global_step,\n",
        "        'state_dict': model.state_dict(),\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'optimizer' : optimizer.state_dict(),\n",
        "    }\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(checkpoint, 'densenet_bc_100_12_bestacc_12_classes.pth')\n",
        "        #torch.save(model, '%d-%s-best-loss.pth' % (start_timestamp, full_name))\n",
        "    if epoch_loss < best_loss:\n",
        "        best_loss = epoch_loss\n",
        "        torch.save(checkpoint, 'densenet_bc_100_12_bestloss_12_classes.pth')\n",
        "        #torch.save(model, '%d-%s-best-acc.pth' % (start_timestamp, full_name))\n",
        "\n",
        "    torch.save(checkpoint, 'densenet_bc_100_12_resume_12_classes.pth')\n",
        "    del checkpoint  # reduce memory\n",
        "\n",
        "    return epoch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJlcrJLcaO4v"
      },
      "source": [
        "print(\"training...\")\n",
        "since = time.time()\n",
        "for epoch in range(start_epoch, 70):\n",
        "    #if args.lr_scheduler == 'step':\n",
        "        #lr_scheduler.step()\n",
        "\n",
        "    train(epoch)\n",
        "    epoch_loss = valid(epoch)\n",
        "\n",
        "    #if args.lr_scheduler == 'plateau':\n",
        "    lr_scheduler.step(metrics=epoch_loss)\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    time_str = 'total time elapsed: {:.0f}h {:.0f}m {:.0f}s '.format(time_elapsed // 3600, time_elapsed % 3600 // 60, time_elapsed % 60)\n",
        "    #print(\"%s, best accuracy: %.02f%%, best loss %f\" % (time_str, 100*best_accuracy, best_loss))\n",
        "print(\"finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyhxV9EFlD0E"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}