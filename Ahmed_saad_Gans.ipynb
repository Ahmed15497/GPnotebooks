{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_samples_GAN.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mky7AX4no25-"
      },
      "source": [
        "# Important Packages, framework versions, and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uXbnOPVO-gu"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqvJIyxqo4JF"
      },
      "source": [
        "# to use tensorflow of version 1. in colab (colab supports two versions \n",
        "# to help us in hparams which include constants needed in spectrogram and inv_spectrogram functions)\n",
        "%tensorflow_version 1.x "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFg4z6X5o6zm"
      },
      "source": [
        "pip install hparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur4f-HyUo8Vo"
      },
      "source": [
        "import tensorflow as tf\n",
        "from hparams import hparams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAajwPoBqtrb"
      },
      "source": [
        "functions for loading and saving audios, spectrogram and inv_spectrogram functions, normalize and denormalize, griffin_lim for constructing phase, stft and istft "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW55F_otpCl6"
      },
      "source": [
        "def save_wav(wav, path):\n",
        "    wav=np.int16(wav/np.max(np.abs(wav))*32768)\n",
        "    if len(wav)>16000: \n",
        "      wav=wav[0:16000]\n",
        "      wavfile.write(path, hparams.sample_rate, wav)\n",
        "    elif len(wav)==16000: \n",
        "      wavfile.write(path, hparams.sample_rate, wav)\n",
        "    #librosa.output.write_wav(path, wav, hparams.sample_rate)\n",
        "\n",
        "def spectrogram(y):\n",
        "    D = _stft(_preemphasis(y))\n",
        "    S = _amp_to_db(np.abs(D)) - hparams.ref_level_db\n",
        "    return _normalize(S)\n",
        "\n",
        "\n",
        "def inv_spectrogram(spectrogram):\n",
        "    S = _db_to_amp(_denormalize(spectrogram) + hparams.ref_level_db)  # Convert back to linear\n",
        "    return _inv_preemphasis(_griffin_lim(S ** 1.5))  # Reconstruct phase\n",
        "\n",
        "\n",
        "def melspectrogram(y):\n",
        "    D = _stft(_preemphasis(y))\n",
        "    S = _amp_to_db(_linear_to_mel(np.abs(D)))\n",
        "    return _normalize(S)\n",
        "\n",
        "\n",
        "def inv_melspectrogram(melspectrogram):\n",
        "    S = _mel_to_linear(_db_to_amp(_denormalize(melspectrogram)))  # Convert back to linear\n",
        "    return _inv_preemphasis(_griffin_lim(S ** 1.5))  # Reconstruct phase\n",
        "\n",
        "\n",
        "# Based on https://github.com/librosa/librosa/issues/434\n",
        "def _griffin_lim(S):\n",
        "    angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n",
        "    S_complex = np.abs(S).astype(np.complex)\n",
        "    for i in range(hparams.griffin_lim_iters):\n",
        "        if i > 0:\n",
        "            angles = np.exp(1j * np.angle(_stft(y)))\n",
        "        y = _istft(S_complex * angles)\n",
        "    return y\n",
        "\n",
        "\n",
        "def _stft(y):\n",
        "    n_fft = (hparams.num_freq - 1) * 2\n",
        "    hop_length = int(hparams.frame_shift_ms / 1000. * hparams.sample_rate)\n",
        "    win_length = int(hparams.frame_length_ms / 1000. * hparams.sample_rate)\n",
        "    return librosa.stft(y=y, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
        "\n",
        "\n",
        "def _istft(y):\n",
        "    hop_length = int(hparams.frame_shift_ms / 1000. * hparams.sample_rate)\n",
        "    win_length = int(hparams.frame_length_ms / 1000. * hparams.sample_rate)\n",
        "    return librosa.istft(y, hop_length=hop_length, win_length=win_length)\n",
        "\n",
        "\n",
        "# Conversions:\n",
        "_mel_basis = None\n",
        "_inv_mel_basis = None\n",
        "\n",
        "def _linear_to_mel(spectrogram):\n",
        "    global _mel_basis\n",
        "    if _mel_basis is None:\n",
        "        _mel_basis = _build_mel_basis()\n",
        "    return np.dot(_mel_basis, spectrogram)\n",
        "\n",
        "def _mel_to_linear(mel_spectrogram):\n",
        "    global _inv_mel_basis\n",
        "    if _inv_mel_basis is None:\n",
        "        _inv_mel_basis = np.linalg.pinv(_build_mel_basis())\n",
        "    return np.maximum(1e-10, np.dot(_inv_mel_basis, mel_spectrogram))\n",
        "\n",
        "def _build_mel_basis():\n",
        "    n_fft = (hparams.num_freq - 1) * 2\n",
        "    return librosa.filters.mel(hparams.sample_rate, n_fft, n_mels=hparams.num_mels)\n",
        "\n",
        "def _amp_to_db(x):\n",
        "    return 20 * np.log10(np.maximum(1e-5, x))\n",
        "\n",
        "def _db_to_amp(x):\n",
        "    return np.power(10.0, x * 0.05)\n",
        "\n",
        "\n",
        "def _preemphasis(x):\n",
        "    return signal.lfilter([1, -hparams.preemphasis], [1], x)\n",
        "\n",
        "\n",
        "def _inv_preemphasis(x):\n",
        "    return signal.lfilter([1], [1, -hparams.preemphasis], x)\n",
        "\n",
        "def _normalize(S):\n",
        "    return np.clip((S - hparams.min_level_db) / -float(hparams.min_level_db), 0, 1)\n",
        "\n",
        "def _denormalize(S):\n",
        "  return (np.clip(S, 0, 1) * -hparams.min_level_db) + hparams.min_level_db"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TP0Qt1hrL14"
      },
      "source": [
        "constants to make the conversion of spectogram and inv_spectrogram most audio information "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLHLfXFopF7v"
      },
      "source": [
        "hparams = tf.contrib.training.HParams(\n",
        "    num_mels=128,\n",
        "    num_freq=1013,\n",
        "    sample_rate=16000,\n",
        "    frame_length_ms=16.0,\n",
        "    frame_shift_ms=8.0,\n",
        "    preemphasis=0.97,\n",
        "    min_level_db=-80,\n",
        "    ref_level_db=20,\n",
        "\n",
        "    griffin_lim_iters=60\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu9XjbHFpJyw"
      },
      "source": [
        "# Google Drive Mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPPT_eTEpLCs"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHXVhRX-tIm0"
      },
      "source": [
        "to get .npz file needed for normalizing and denormalizing spectrogram and inv_spectrogram "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99V9LcmYtD95"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Nwishy/GAN_10_classes/melspectrogram_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQjoURijpRek"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import torch\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "from librosa.display import specshow\n",
        "from scipy.io import wavfile\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msgmriHNyXmz"
      },
      "source": [
        "DATAPATH = \"training_data.npz\"\n",
        "\n",
        "CATEGORIES = np.load(DATAPATH)[\"category_names\"]\n",
        "\n",
        "# for denomalizing mel_spectrogram\n",
        "mel_means = np.load(DATAPATH)[\"mean\"]\n",
        "mel_stds = np.load(DATAPATH)[\"std\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w62GVxS8pVRQ"
      },
      "source": [
        "import librosa.filters\n",
        "from scipy import signal\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Reshape, Flatten,InputLayer\n",
        "from keras.layers import Activation, UpSampling2D, Conv2D\n",
        "from keras.layers.merge import _Merge\n",
        "from keras.layers.convolutional import Convolution2D, Conv2DTranspose\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras import backend as K\n",
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq-PbJ8cpYII"
      },
      "source": [
        "try:\n",
        "    from PIL import Image\n",
        "except ImportError:\n",
        "    print('This script depends on pillow! Please install it (e.g. with pip install pillow)')\n",
        "    exit()\n",
        "\n",
        "# a constant in generator and discriminator models\n",
        "D = 64 # model size coef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxUkvB9EsAPn"
      },
      "source": [
        "generator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu7B74gXpelA"
      },
      "source": [
        "def make_generator():\n",
        "    \"\"\"Creates a generator model that takes a 100-dimensional noise vector as a \"seed\", and outputs images\n",
        "    of size 128x128x1.\"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Dense(256 * D, input_dim=100))\n",
        "    model.add(Reshape((4, 4, 16 * D)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(UpSampling2D(size=(2, 2)))\n",
        "    model.add(Conv2D(8 * D, (5, 5), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(UpSampling2D(size=(2, 2)))\n",
        "    model.add(Conv2D(4 * D, (5, 5), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(UpSampling2D(size=(2, 2)))\n",
        "    model.add(Conv2D(2 * D, (5, 5), padding='same'))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(D, (5, 5), padding='same'))\n",
        "    model.add(UpSampling2D(size=(2, 2)))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(UpSampling2D(size=(2, 2)))\n",
        "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
        "    model.add(Activation('tanh'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5AzODxesDGg"
      },
      "source": [
        "discriminator model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnuzrCegphUp"
      },
      "source": [
        "def make_discriminator(nb_categories):\n",
        "    \"\"\" Discriminator to determine if it's real or fake and category of the sound.\n",
        "        Note that unlike normal GANs, the real/fake output is not sigmoid and does not represent a probability\n",
        "     \"\"\"\n",
        "\n",
        "    input_data = Input(shape=(128, 128, 1))\n",
        "    x = Conv2D(D, (5, 5), strides=(2,2), padding='same')(input_data)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(D * 2, (5, 5), strides=(2,2), kernel_initializer='he_normal',padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(D * 4, (5, 5), strides=(2,2), kernel_initializer='he_normal',padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(D * 8, (5, 5), strides=(2,2), kernel_initializer='he_normal',padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Conv2D(D * 16, (5, 5), strides=(2,2), kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = LeakyReLU(alpha=0.2)(x)\n",
        "    x = Flatten()(x)\n",
        "    real_fake = Dense(1, kernel_initializer='he_normal', name='real_fake')(x) # no activation for wasserstein_loss\n",
        "    categories = Dense(nb_categories, kernel_initializer='he_normal', name='categories', activation='softmax')(x)\n",
        "\n",
        "    model = Model(input_data, [real_fake, categories])\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHpN52z_sZfj"
      },
      "source": [
        "to save audio generated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59MW3Gxnpmcg"
      },
      "source": [
        "def save_audio(y, path):\n",
        "    \"\"\" generate a wav file from a given spectrogram and save it \"\"\"\n",
        "    s = np.squeeze(y)\n",
        "    s = denormalize(s)\n",
        "    w = inv_melspectrogram(s)\n",
        "    save_wav(w, path)\n",
        "\n",
        "def denormalize(norm_s):\n",
        "    \"\"\" normalized spectrogram to original spectrogram using the calculated mean/standard deviation \"\"\"\n",
        "    assert norm_s.shape[0] == mel_means.shape[0]\n",
        "    Y = (norm_s * (3.0 * mel_stds)) + mel_means\n",
        "    return Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkmhzK2NspCG"
      },
      "source": [
        "initialize models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO_mXflSy8J0"
      },
      "source": [
        "nb_categories = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cneQueKpqQ4"
      },
      "source": [
        "generator = make_generator()\n",
        "discriminator = make_discriminator(nb_categories)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAieeJTGssQC"
      },
      "source": [
        "to get saved models .h5 file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-4HFmQUptQe"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Nwishy/GAN_10_classes/GAN_Outputs/Generated_Images_and_Audios"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhndfBAJptuI"
      },
      "source": [
        "generator.load_weights('generator_epoch_844_0.858.h5')\n",
        "discriminator.load_weights('discriminator_epoch_844_-1.24.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR82mOVusyO4"
      },
      "source": [
        "if you want to save in drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qiGEuyppwlh"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Nwishy/GAN_10_classes/GAN_Outputs/generate_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t36kcqS4p49S"
      },
      "source": [
        "import librosa\n",
        "from librosa import display\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython\n",
        "from IPython.display import Audio\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# check if the discriminator thinks the generated sound as real sound\n",
        "CONFIDENCE_THRESH = 0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTNqh-_QtkIy"
      },
      "source": [
        "function to classify audio generated 128x128x1 spectrogram image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uea7ADvKp7ry"
      },
      "source": [
        "def classify_generated(w, thresh=CONFIDENCE_THRESH):\n",
        "    # prepare image to be input to discriminator\n",
        "    w = np.squeeze(w)\n",
        "    w = w[np.newaxis, :, :, np.newaxis]\n",
        "    # will return r which is real or fake and p probability array of categories \n",
        "    r, p = discriminator.predict([w])\n",
        "    # r must be greater than thresh to get sample if not will return -1 \n",
        "    if float(r) > thresh:\n",
        "        return np.argmax(p)\n",
        "    else:\n",
        "        return -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKYJnhuCrAb2"
      },
      "source": [
        "function for checking generated samples category"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76dBLxQf6A3Q"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import init\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import librosa\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00XeUHh--chU"
      },
      "source": [
        "num_classes = 12\n",
        "in_channels = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j5GDN75DYo6"
      },
      "source": [
        "use_gpu = torch.cuda.is_available()\n",
        "print('use_gpu', use_gpu)\n",
        "if use_gpu:\n",
        "    torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-yWhDcqNaYS"
      },
      "source": [
        "classes=  ['down',\n",
        "          'go',\n",
        "          'left',\n",
        "          'no',\n",
        "          'off',\n",
        "          'on',\n",
        "          'right',   \n",
        "          'silence',    \n",
        "          'stop',\n",
        "          'unknown',\n",
        "          'up',   \n",
        "          'yes']    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCviEsIO94T3"
      },
      "source": [
        "models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUpkl7t1s5zF"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Nwishy/transfer_learning/densenet_bc_100_12_librosa_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnjP1plbCUZS"
      },
      "source": [
        "__all__ = [ 'DenseNet' ]\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, inplanes, expansion=4, growthRate=12, dropRate=0):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        planes = expansion * growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropRate = dropRate\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        if self.dropRate > 0:\n",
        "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
        "\n",
        "        out = torch.cat((x, out), 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, expansion=1, growthRate=12, dropRate=0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        planes = expansion * growthRate\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, growthRate, kernel_size=3,\n",
        "                               padding=1, bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropRate = dropRate\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        if self.dropRate > 0:\n",
        "            out = F.dropout(out, p=self.dropRate, training=self.training)\n",
        "\n",
        "        out = torch.cat((x, out), 1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, inplanes, outplanes):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(inplanes)\n",
        "        self.conv1 = nn.Conv2d(inplanes, outplanes, kernel_size=1,\n",
        "                               bias=False)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.bn1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = F.avg_pool2d(out, 2)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "\n",
        "    def __init__(self, depth=22, block=Bottleneck,\n",
        "        dropRate=0, num_classes=10, growthRate=12, compressionRate=2, in_channels=3):\n",
        "        super(DenseNet, self).__init__()\n",
        "\n",
        "        assert (depth - 4) % 3 == 0, 'depth should be 3n+4'\n",
        "        n = (depth - 4) / 3 if block == BasicBlock else (depth - 4) // 6\n",
        "\n",
        "        self.growthRate = growthRate\n",
        "        self.dropRate = dropRate\n",
        "\n",
        "        # self.inplanes is a global variable used across multiple\n",
        "        # helper functions\n",
        "        self.inplanes = growthRate * 2\n",
        "        self.conv1 = nn.Conv2d(in_channels, self.inplanes, kernel_size=3, padding=1,\n",
        "                               bias=False)\n",
        "        self.dense1 = self._make_denseblock(block, n)\n",
        "        self.trans1 = self._make_transition(compressionRate)\n",
        "        self.dense2 = self._make_denseblock(block, n)\n",
        "        self.trans2 = self._make_transition(compressionRate)\n",
        "        self.dense3 = self._make_denseblock(block, n)\n",
        "        self.bn = nn.BatchNorm2d(self.inplanes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.avgpool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(self.inplanes, num_classes)\n",
        "\n",
        "        # Weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    def _make_denseblock(self, block, blocks):\n",
        "        layers = []\n",
        "        for i in range(blocks):\n",
        "            # Currently we fix the expansion ratio as the default value\n",
        "            layers.append(block(self.inplanes, growthRate=self.growthRate, dropRate=self.dropRate))\n",
        "            self.inplanes += self.growthRate\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _make_transition(self, compressionRate):\n",
        "        inplanes = self.inplanes\n",
        "        outplanes = int(math.floor(self.inplanes // compressionRate))\n",
        "        self.inplanes = outplanes\n",
        "        return Transition(inplanes, outplanes)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "\n",
        "        x = self.trans1(self.dense1(x))\n",
        "        x = self.trans2(self.dense2(x))\n",
        "        x = self.dense3(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AazhEeCLs7Gh"
      },
      "source": [
        "MODEL_NAME1 = 'DenseNet_scipy_12_classes_semi_supervised'\n",
        "\n",
        "modelA = DenseNet(depth=100, growthRate=12, compressionRate=2, num_classes=num_classes, in_channels=in_channels)\n",
        "if use_gpu:\n",
        "    modelA = torch.nn.DataParallel(modelA).cuda()\n",
        "\n",
        "checkpoint = torch.load(f'{MODEL_NAME1}_bestloss.pth')\n",
        "modelA.load_state_dict(checkpoint['state_dict'])\n",
        "modelA.float()    \n",
        "del checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syoqjwfztC8T"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Nwishy/transfer_learning/wide_resnet_librosa_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ2qNxl4AqjR"
      },
      "source": [
        "__all__ = [ 'WideResNet' ]\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        self.droprate = dropRate\n",
        "        self.equalInOut = (in_planes == out_planes)\n",
        "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
        "                               padding=0, bias=False) or None\n",
        "    def forward(self, x):\n",
        "        if not self.equalInOut:\n",
        "            x = self.relu1(self.bn1(x))\n",
        "        else:\n",
        "            out = self.relu1(self.bn1(x))\n",
        "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
        "        if self.droprate > 0:\n",
        "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
        "        out = self.conv2(out)\n",
        "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
        "\n",
        "class NetworkBlock(nn.Module):\n",
        "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
        "        super(NetworkBlock, self).__init__()\n",
        "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
        "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
        "        layers = []\n",
        "        for i in range(nb_layers):\n",
        "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
        "        return nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.layer(x)\n",
        "\n",
        "class WideResNet(nn.Module):\n",
        "    def __init__(self, depth, num_classes, in_channels=3, widen_factor=1, dropRate=0.0):\n",
        "        super(WideResNet, self).__init__()\n",
        "        nChannels = [16, 16*widen_factor, 32*widen_factor, 64*widen_factor]\n",
        "        assert((depth - 4) % 6 == 0)\n",
        "        n = (depth - 4) // 6\n",
        "        block = BasicBlock\n",
        "        # 1st conv before any network block\n",
        "        self.conv1 = nn.Conv2d(in_channels, nChannels[0], kernel_size=3, stride=1,\n",
        "                               padding=1, bias=False)\n",
        "        # 1st block\n",
        "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
        "        # 2nd block\n",
        "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
        "        # 3rd block\n",
        "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
        "        # global average pooling and classifier\n",
        "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
        "        self.nChannels = nChannels[3]\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                m.bias.data.zero_()\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.block1(out)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.relu(self.bn1(out))\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "        out = out.view(-1, self.nChannels)\n",
        "        return self.fc(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AovCre1ftDV4"
      },
      "source": [
        "MODEL_NAME2 = 'WideResNet_scipy_12_classes'\n",
        "\n",
        "modelB = WideResNet(depth=52, widen_factor=10, dropRate=0, num_classes=num_classes, in_channels=in_channels)\n",
        "if use_gpu:\n",
        "    modelB = torch.nn.DataParallel(modelB).cuda()\n",
        "\n",
        "checkpoint = torch.load(f'{MODEL_NAME2}_bestloss.pth')\n",
        "modelB.load_state_dict(checkpoint['state_dict'])\n",
        "modelB.float() \n",
        "del checkpoint       "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tvToQH1tFE4"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Dataset/transfer_learning/playground"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4w207jVCpcG"
      },
      "source": [
        "class ResNeXtBottleneck(nn.Module):\n",
        "    \"\"\"\n",
        "    RexNeXt bottleneck type C (https://github.com/facebookresearch/ResNeXt/blob/master/models/resnext.lua)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride, cardinality, base_width, widen_factor):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            in_channels: input channel dimensionality\n",
        "            out_channels: output channel dimensionality\n",
        "            stride: conv stride. Replaces pooling layer.\n",
        "            cardinality: num of convolution groups.\n",
        "            base_width: base number of channels in each group.\n",
        "            widen_factor: factor to reduce the input dimensionality before convolution.\n",
        "        \"\"\"\n",
        "        super(ResNeXtBottleneck, self).__init__()\n",
        "        width_ratio = out_channels / (widen_factor * 64.)\n",
        "        D = cardinality * int(base_width * width_ratio)\n",
        "        self.conv_reduce = nn.Conv2d(in_channels, D, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn_reduce = nn.BatchNorm2d(D)\n",
        "        self.conv_conv = nn.Conv2d(D, D, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(D)\n",
        "        self.conv_expand = nn.Conv2d(D, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
        "        self.bn_expand = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut.add_module('shortcut_conv',\n",
        "                                     nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=0,\n",
        "                                               bias=False))\n",
        "            self.shortcut.add_module('shortcut_bn', nn.BatchNorm2d(out_channels))\n",
        "\n",
        "    def forward(self, x):\n",
        "        bottleneck = self.conv_reduce.forward(x)\n",
        "        bottleneck = F.relu(self.bn_reduce.forward(bottleneck), inplace=True)\n",
        "        bottleneck = self.conv_conv.forward(bottleneck)\n",
        "        bottleneck = F.relu(self.bn.forward(bottleneck), inplace=True)\n",
        "        bottleneck = self.conv_expand.forward(bottleneck)\n",
        "        bottleneck = self.bn_expand.forward(bottleneck)\n",
        "        residual = self.shortcut.forward(x)\n",
        "        return F.relu(residual + bottleneck, inplace=True)\n",
        "\n",
        "\n",
        "class CifarResNeXt(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNext optimized for the Cifar dataset, as specified in\n",
        "    https://arxiv.org/pdf/1611.05431.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, nlabels, cardinality=8, depth=29, base_width=64, widen_factor=4, in_channels=3):\n",
        "        \"\"\" Constructor\n",
        "        Args:\n",
        "            cardinality: number of convolution groups.\n",
        "            depth: number of layers.\n",
        "            nlabels: number of classes\n",
        "            base_width: base number of channels in each group.\n",
        "            widen_factor: factor to adjust the channel dimensionality\n",
        "        \"\"\"\n",
        "        super(CifarResNeXt, self).__init__()\n",
        "        self.cardinality = cardinality\n",
        "        self.depth = depth\n",
        "        self.block_depth = (self.depth - 2) // 9\n",
        "        self.base_width = base_width\n",
        "        self.widen_factor = widen_factor\n",
        "        self.nlabels = nlabels\n",
        "        self.output_size = 64\n",
        "        self.stages = [64, 64 * self.widen_factor, 128 * self.widen_factor, 256 * self.widen_factor]\n",
        "\n",
        "        self.conv_1_3x3 = nn.Conv2d(in_channels, 64, 3, 1, 1, bias=False)\n",
        "        self.bn_1 = nn.BatchNorm2d(64)\n",
        "        self.stage_1 = self.block('stage_1', self.stages[0], self.stages[1], 1)\n",
        "        self.stage_2 = self.block('stage_2', self.stages[1], self.stages[2], 2)\n",
        "        self.stage_3 = self.block('stage_3', self.stages[2], self.stages[3], 2)\n",
        "        self.classifier = nn.Linear(self.stages[3], nlabels)\n",
        "        init.kaiming_normal(self.classifier.weight)\n",
        "\n",
        "        for key in self.state_dict():\n",
        "            if key.split('.')[-1] == 'weight':\n",
        "                if 'conv' in key:\n",
        "                    init.kaiming_normal(self.state_dict()[key], mode='fan_out')\n",
        "                if 'bn' in key:\n",
        "                    self.state_dict()[key][...] = 1\n",
        "            elif key.split('.')[-1] == 'bias':\n",
        "                self.state_dict()[key][...] = 0\n",
        "\n",
        "    def block(self, name, in_channels, out_channels, pool_stride=2):\n",
        "        \"\"\" Stack n bottleneck modules where n is inferred from the depth of the network.\n",
        "        Args:\n",
        "            name: string name of the current block.\n",
        "            in_channels: number of input channels\n",
        "            out_channels: number of output channels\n",
        "            pool_stride: factor to reduce the spatial dimensionality in the first bottleneck of the block.\n",
        "        Returns: a Module consisting of n sequential bottlenecks.\n",
        "        \"\"\"\n",
        "        block = nn.Sequential()\n",
        "        for bottleneck in range(self.block_depth):\n",
        "            name_ = '%s_bottleneck_%d' % (name, bottleneck)\n",
        "            if bottleneck == 0:\n",
        "                block.add_module(name_, ResNeXtBottleneck(in_channels, out_channels, pool_stride, self.cardinality,\n",
        "                                                          self.base_width, self.widen_factor))\n",
        "            else:\n",
        "                block.add_module(name_,\n",
        "                                 ResNeXtBottleneck(out_channels, out_channels, 1, self.cardinality, self.base_width,\n",
        "                                                   self.widen_factor))\n",
        "        return block\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_1_3x3.forward(x)\n",
        "        x = F.relu(self.bn_1.forward(x), inplace=True)\n",
        "        x = self.stage_1.forward(x)\n",
        "        x = self.stage_2.forward(x)\n",
        "        x = self.stage_3.forward(x)\n",
        "        x = F.avg_pool2d(x, 8, 1)\n",
        "        x = x.view(-1, self.stages[3])\n",
        "        return self.classifier(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y97KjDttGx4"
      },
      "source": [
        "MODEL_NAME3 = 'Resnextscipyv2'\n",
        "\n",
        "modelC = CifarResNeXt(nlabels=num_classes, in_channels=in_channels)\n",
        "if use_gpu:\n",
        "    modelC = torch.nn.DataParallel(modelC).cuda()\n",
        "\n",
        "checkpoint = torch.load(f'{MODEL_NAME3}_bestloss.pth')\n",
        "modelC.load_state_dict(checkpoint['state_dict'])\n",
        "modelC.float() \n",
        "del checkpoint  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHpRKmDcC1yh"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.out_planes = out_planes\n",
        "        self.dense_depth = dense_depth\n",
        "\n",
        "        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
        "        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if first_layer:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_planes+dense_depth)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        x = self.shortcut(x)\n",
        "        d = self.out_planes\n",
        "        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class DPN(nn.Module):\n",
        "    def __init__(self, num_classes, in_channels, cfg):\n",
        "        super(DPN, self).__init__()\n",
        "        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n",
        "        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.last_planes = 64\n",
        "        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n",
        "        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n",
        "        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n",
        "        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n",
        "        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], num_classes)\n",
        "\n",
        "    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for i,stride in enumerate(strides):\n",
        "            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n",
        "            self.last_planes = out_planes + (i+2) * dense_depth\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npdAqMZ2C7Sv"
      },
      "source": [
        "def DPN92(num_classes, in_channels=1):\n",
        "    cfg = {\n",
        "        'in_planes': (96,192,384,768),\n",
        "        'out_planes': (256,512,1024,2048),\n",
        "        'num_blocks': (3,4,20,3),\n",
        "        'dense_depth': (16,32,24,128)\n",
        "    }\n",
        "    return DPN(num_classes, in_channels, cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n91nJMMntIw3"
      },
      "source": [
        "MODEL_NAME4 = 'DPN92'\n",
        "\n",
        "modelD = DPN92(num_classes=num_classes, in_channels=in_channels)\n",
        "if use_gpu:\n",
        "    modelD = torch.nn.DataParallel(modelD).cuda()\n",
        "\n",
        "checkpoint = torch.load(f'{MODEL_NAME4}_bestloss.pth')\n",
        "modelD.load_state_dict(checkpoint['state_dict'])\n",
        "modelD.float() \n",
        "del checkpoint      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drsAL95svacI"
      },
      "source": [
        "def ToTensor(data):\n",
        "  \"\"\"Converts into a tensor.\"\"\"\n",
        "  tensor = torch.FloatTensor(data)\n",
        "  return tensor\n",
        "\n",
        "def ToSTFT(data,n_fft=2048, hop_length=512):\n",
        "  \"\"\"Applies on an audio the short time fourier transform.\"\"\"\n",
        "  data_stft = librosa.stft(data, n_fft=n_fft, hop_length=hop_length)\n",
        "  return data_stft   \n",
        "\n",
        "def ToMFCC(data,n_mels=32,sample_rate=16000,n_fft=2048):\n",
        "  \"\"\"Creates the mel spectrogram from the short time fourier transform of a file. The result is a 32x32 matrix.\"\"\"\n",
        "  data = data.astype(np.float32, order='C') / 32768.0\n",
        "  stft = ToSTFT(data)\n",
        "  mel_basis = librosa.filters.mel(sample_rate, n_fft, n_mels)\n",
        "  s = np.dot(mel_basis, np.abs(stft)**2.0)\n",
        "  data_mfcc = librosa.power_to_db(s, ref=np.max)\n",
        "  data_mfcc = ToTensor(data_mfcc)\n",
        "  return data_mfcc \n",
        "\n",
        "def FixAudioLength(data,time = 1,sample_rate = 16000):\n",
        "    \"\"\"fixes audio length to be 16000 sample.\"\"\"\n",
        "    length = int(time * sample_rate)\n",
        "    if length < len(data):\n",
        "        data = data[:length]\n",
        "    elif length > len(data):\n",
        "        data = np.pad(data, (0, length - len(data)), \"constant\")\n",
        "    return data  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLuSjJlYrBn4"
      },
      "source": [
        "def check_samples_category(wav_vector):\n",
        "  ''' checking sample category '''\n",
        "\n",
        "  # fix audio length to be 16000 samples\n",
        "  wav_vector = FixAudioLength(wav_vector)\n",
        "  # convert wav_audio to mel-spectrogram to be ready for CNN models \n",
        "  input = ToMFCC(wav_vector)\n",
        "  # Set model to evaluate mode\n",
        "  modelA.eval()  \n",
        "  modelB.eval()\n",
        "  modelC.eval()\n",
        "  modelD.eval()\n",
        "  # prepare input\n",
        "  input = torch.unsqueeze(input, 0)\n",
        "  input = torch.unsqueeze(input, 1)\n",
        "  input = input.float()\n",
        "  with torch.no_grad():\n",
        "    if use_gpu:\n",
        "        input = input.cuda()\n",
        "  # models predictions\n",
        "  outputsA = modelA(input)\n",
        "  outputsB = modelB(input)\n",
        "  outputsC = modelC(input)\n",
        "  outputsD = modelD(input)\n",
        "  outputs_CNN = outputsA + outputsB + outputsC + outputsD\n",
        "  outputs = torch.nn.functional.softmax(outputs_CNN, dim=1)\n",
        "  # get index of highest probability\n",
        "  pred = outputs.data.max(1, keepdim=True)[1]\n",
        "  # get predicted class\n",
        "  predicted_class = classes[pred]\n",
        "  return predicted_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3YVBzNCqu2X"
      },
      "source": [
        "# **GAN run**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGVu4fPkrmFQ"
      },
      "source": [
        "CATEGORIES"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUxUzYJCrov_"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Dataset/dataset_gan/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIYCNOM0VyFI"
      },
      "source": [
        "folder = \"dataset_part90\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIlcifc-WIFZ"
      },
      "source": [
        "!mkdir $folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tHpoxHnrs-n"
      },
      "source": [
        "import os\n",
        "for cat in CATEGORIES:\n",
        "    os.mkdir(os.path.join(folder,cat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJZMVvT8p-Tr"
      },
      "source": [
        "def generate_random_sound(folder, max_try= 1000):\n",
        "    i = 0\n",
        "    while i < max_try:\n",
        "        seed = np.random.rand(1, 100)              # random vector \n",
        "        w =   generator.predict(seed)              # generated 128x128x1            \n",
        "        id_category = classify_generated(w[0])     # real or not and belong to much category\n",
        "        category =  CATEGORIES[id_category]        # get name of class (right or left or no or off ...)\n",
        "        ###############################################################################################\n",
        "        # process w to be int16 vect\n",
        "        s = np.squeeze(w)\n",
        "        s = denormalize(s)\n",
        "        wav = inv_melspectrogram(s)\n",
        "        wav = np.int16(wav/np.max(np.abs(wav))*32768)\n",
        "        wav = wav[0:16000]\n",
        "        prediction = check_samples_category(wav)\n",
        "        ###############################################################################################\n",
        "        # id_category is what  classify_generated returns and must be > 0 to get the generated sample\n",
        "        if  id_category >= 0 and prediction == category:\n",
        "            save_audio(w, f\"{folder}/%s/sample_%d.wav\" %(category, i))\n",
        "            i += 1\n",
        "            if i % 400 == 0:\n",
        "              print(f\"sample number {i}\")            \n",
        "            #print(category)\n",
        "        #else: \n",
        "          #print('ok!!!!!')     # if the generated sample can't fool the discriminator\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qu1X2PQ40L2F"
      },
      "source": [
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywqQbcvSrUD1"
      },
      "source": [
        "start = time.time()\n",
        "generate_random_sound(folder, 2500)\n",
        "end = time.time()\n",
        "print( (end - start) / (60*60) ) \n",
        "           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0tdFS-Wu0xh"
      },
      "source": [
        "#from scipy.io import wavfile      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_1sWBRvu5-2"
      },
      "source": [
        "#fs, samples = wavfile.read('dataset/up/sample_70.wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKoTebOvvJfS"
      },
      "source": [
        "#check_samples_category(samples)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DvuFhavURAF"
      },
      "source": [
        "#folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqD45-yioqf5"
      },
      "source": [
        "#!mv dataset dataset_part56"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Tag7N9DvcOi"
      },
      "source": [
        "!zip -r $folder $folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMQsuCtyZrhb"
      },
      "source": [
        "!zip -r dataset_part74 dataset_part74\n",
        "!rm -r dataset_part74"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eSqhclKUePj"
      },
      "source": [
        "!rm -r $folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbbGShofo27G"
      },
      "source": [
        "#!cp dataset_part56.zip /content/drive/My\\ Drive/NLP_Dataset/dataset_gan/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NI6f1MKQqNaC"
      },
      "source": [
        "import os\n",
        "for dir in os.listdir(f'/content/drive/My Drive/NLP_Dataset/dataset_gan/{folder}'):\n",
        "  path = f\"/content/drive/My Drive/NLP_Dataset/dataset_gan/{folder}/{dir}\"\n",
        "  audios = os.listdir(path)\n",
        "  print(f\"{dir} : {len(audios)}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4neJ_T3Oz6l"
      },
      "source": [
        "**ushould copy the dataset_zipped to the drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4edu-XowZWJ"
      },
      "source": [
        "#!rm -r dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njThrIrQbFa-"
      },
      "source": [
        "trying the function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0dFyAXCEpap"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/NLP_Nwishy/GAN_10_classes/GAN_Outputs/generate_samples/down"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc7fYCBaEsU4"
      },
      "source": [
        "path = 'down'\n",
        "waves = [f for f in os.listdir(path) if f.endswith('.wav')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw-ESD_eGIP1"
      },
      "source": [
        "counter = 0\n",
        "for f in range(len(waves)):\n",
        "  sample_rate, data = wavfile.read(waves[f], 16000)\n",
        "  prediction = check_samples_category(data)\n",
        "  if prediction == 'down':\n",
        "    counter += 1\n",
        "  print((waves[f],prediction))\n",
        "print(counter) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnNsYEL_JHRS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}